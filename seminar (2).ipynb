{"nbformat":4,"nbformat_minor":4,"metadata":{"notebookPath":"seminar.ipynb","accelerator":"GPU","language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.7.7","file_extension":".py"},"notebookId":"f7689ddf-57b2-462d-8a86-b738ceba5497","colab":{"name":"seminar.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"markdown","source":"# Import and misc","metadata":{"cellId":"olfinl5wndknokfpgf7tb","id":"_lhrn5O-qUYZ"}},{"cell_type":"code","source":"%pip install torchaudio==0.9.1","metadata":{"outputId":"b81351fe-e4ec-4de2-b080-fc0725f3ba71","id":"meO-Mp9jiAFC","cellId":"t1obfz1fm8g43yhzpr7u2j","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchaudio==0.9.1 in /home/jupyter/.local/lib/python3.7/site-packages (0.9.1)\nRequirement already satisfied: torch==1.9.1 in /home/jupyter/.local/lib/python3.7/site-packages (from torchaudio==0.9.1) (1.9.1)\nRequirement already satisfied: typing-extensions in /kernel/lib/python3.7/site-packages (from torch==1.9.1->torchaudio==0.9.1) (4.0.0)\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"}],"execution_count":662},{"cell_type":"code","source":"from typing import Tuple, Union, List, Callable, Optional\nfrom tqdm import tqdm\nfrom itertools import islice\nimport pathlib\nimport dataclasses\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch import distributions\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport torchaudio\nfrom IPython import display as display_","metadata":{"id":"bbUpoArCqUYa","cellId":"jzgrdb3l19qreux8jqka","trusted":true},"outputs":[],"execution_count":661},{"cell_type":"code","source":"print('hello')","metadata":{"cellId":"a00p2cghk5hybqwvczbrxl","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"hello\n"}],"execution_count":667},{"cell_type":"markdown","source":"# Task","metadata":{"cellId":"n7fn9uqnfohz5fncbq7ilj","id":"812GwLfqqUYf"}},{"cell_type":"markdown","source":"In this notebook we will implement a model for finding a keyword in a stream.\n\nWe will implement the version with CRNN because it is easy and improves the model. \n(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)","metadata":{"cellId":"sxj94ictnyk8lo19r6e7p","id":"i1DuQIyRqUYf"}},{"cell_type":"code","source":"#!g1.1\n@dataclasses.dataclass\nclass TaskConfig:\n    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n    batch_size: int = 128\n    learning_rate: float = 3e-4\n    weight_decay: float = 1e-5\n    num_epochs: int = 20\n    n_mels: int = 40\n    cnn_out_channels: int = 8\n    kernel_size: Tuple[int, int] = (5, 20)\n    stride: Tuple[int, int] = (2, 8)\n    hidden_size: int = 64\n    gru_num_layers: int = 2\n    bidirectional: bool = False\n    num_classes: int = 2\n    sample_rate: int = 16000\n    device: torch.device = torch.device(\n        'cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"8PdhApeEh9pH","cellId":"upsuss8fb39e5eefbq0j18","trusted":true},"outputs":[],"execution_count":668},{"cell_type":"markdown","source":"# Data","metadata":{"cellId":"7oj0p2qj0bcirzlzvbmb2e","id":"KA1gPmE1h9pI"}},{"cell_type":"code","source":"!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz","metadata":{"outputId":"7f8235e9-e2dd-4e33-cabe-07b92c0f2c36","id":"Y2N8zcx9MF1X","cellId":"s68pgd8upklavo9hr3izgh","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log","metadata":{"cellId":"xq0fkqg6rroji5baop5rc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nclass SpeechCommandDataset(Dataset):\n\n    def __init__(\n        self,\n        transform: Optional[Callable] = None,\n        path2dir: str = None,\n        keywords: Union[str, List[str]] = None,\n        csv: Optional[pd.DataFrame] = None\n    ):        \n        self.transform = transform\n\n        if csv is None:\n            path2dir = pathlib.Path(path2dir)\n            keywords = keywords if isinstance(keywords, list) else [keywords]\n            \n            all_keywords = [\n                p.stem for p in path2dir.glob('*')\n                if p.is_dir() and not p.stem.startswith('_')\n            ]\n\n            triplets = []\n            for keyword in all_keywords:\n                paths = (path2dir / keyword).rglob('*.wav')\n                if keyword in keywords:\n                    for path2wav in paths:\n                        triplets.append((path2wav.as_posix(), keyword, 1))\n                else:\n                    for path2wav in paths:\n                        triplets.append((path2wav.as_posix(), keyword, 0))\n            \n            self.csv = pd.DataFrame(\n                triplets,\n                columns=['path', 'keyword', 'label']\n            )\n\n        else:\n            self.csv = csv\n    \n    def __getitem__(self, index: int):\n        instance = self.csv.iloc[index]\n\n        path2wav = instance['path']\n        wav, sr = torchaudio.load(path2wav)\n        wav = wav.sum(dim=0)\n        \n        if self.transform:\n            wav = self.transform(wav)\n\n        return {\n            'wav': wav,\n            'keywors': instance['keyword'],\n            'label': instance['label']\n        }\n\n    def __len__(self):\n        return len(self.csv)","metadata":{"id":"12wBTK0mNUsG","cellId":"y5e9llixp6shhbwtpdqs","trusted":true},"outputs":[],"execution_count":669},{"cell_type":"code","source":"#!g1.1\ndataset = SpeechCommandDataset(\n    path2dir='speech_commands', keywords=TaskConfig.keyword\n)","metadata":{"id":"-1rVkT81Pk90","cellId":"os60r7l9r3j47xojtuabc","trusted":true},"outputs":[],"execution_count":670},{"cell_type":"code","source":"#!g1.1\ndataset.csv.sample(5)","metadata":{"outputId":"ce911e31-70e4-43d9-d5f0-5eca0e4776fc","id":"DFwhAXdfQLIA","cellId":"umuklg2ee9nib9exi4v89","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":204}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                              path keyword  label\n62985  speech_commands/happy/e55a2b20_nohash_1.wav   happy      0\n63500  speech_commands/happy/c50225fa_nohash_1.wav   happy      0\n26216  speech_commands/seven/28ed6bc9_nohash_3.wav   seven      0\n6535     speech_commands/yes/28460a60_nohash_2.wav     yes      0\n48195  speech_commands/right/9ab86dd0_nohash_0.wav   right      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>keyword</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>62985</th>\n      <td>speech_commands/happy/e55a2b20_nohash_1.wav</td>\n      <td>happy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>63500</th>\n      <td>speech_commands/happy/c50225fa_nohash_1.wav</td>\n      <td>happy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26216</th>\n      <td>speech_commands/seven/28ed6bc9_nohash_3.wav</td>\n      <td>seven</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6535</th>\n      <td>speech_commands/yes/28460a60_nohash_2.wav</td>\n      <td>yes</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48195</th>\n      <td>speech_commands/right/9ab86dd0_nohash_0.wav</td>\n      <td>right</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":671},{"cell_type":"markdown","source":"### Augmentations","metadata":{"cellId":"ujqd0ryg0v9a0zh0qpr57k","id":"LUxfDJw1qUYi"}},{"cell_type":"code","source":"#!g1.1\nclass AugsCreation:\n\n    def __init__(self):\n        self.background_noises = [\n            'speech_commands/_background_noise_/white_noise.wav',\n            'speech_commands/_background_noise_/dude_miaowing.wav',\n            'speech_commands/_background_noise_/doing_the_dishes.wav',\n            'speech_commands/_background_noise_/exercise_bike.wav',\n            'speech_commands/_background_noise_/pink_noise.wav',\n            'speech_commands/_background_noise_/running_tap.wav'\n        ]\n\n        self.noises = [\n            torchaudio.load(p)[0].squeeze()\n            for p in self.background_noises\n        ]\n\n    def add_rand_noise(self, audio):\n\n        # randomly choose noise\n        noise_num = torch.randint(low=0, high=len(\n            self.background_noises), size=(1,)).item()\n        noise = self.noises[noise_num]\n\n        noise_level = torch.Tensor([1])  # [0, 40]\n\n        noise_energy = torch.norm(noise)\n        audio_energy = torch.norm(audio)\n        alpha = (audio_energy / noise_energy) * \\\n            torch.pow(10, -noise_level / 20)\n\n        start = torch.randint(\n            low=0,\n            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n            size=(1,)\n        ).item()\n        noise_sample = noise[start: start + audio.size(0)]\n\n        audio_new = audio + alpha * noise_sample\n        audio_new.clamp_(-1, 1)\n        return audio_new\n\n    def __call__(self, wav):\n        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n        augs = [\n            lambda x: x,\n            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n            lambda x: torchaudio.transforms.Vol(.25)(x),\n            lambda x: self.add_rand_noise(x)\n        ]\n\n        return augs[aug_num](wav)","metadata":{"id":"dkmkxPWQqUYe","cellId":"qcwhkcna3sj4wmf8ognayb","trusted":true},"outputs":[],"execution_count":672},{"cell_type":"code","source":"#!g1.1\nindexes = torch.randperm(len(dataset))\ntrain_indexes = indexes[:int(len(dataset) * 0.8)]\nval_indexes = indexes[int(len(dataset) * 0.8):]\n\ntrain_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\nval_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)","metadata":{"id":"ClWThxyYh9pM","cellId":"1budd0nmr4r9y19fshrez","trusted":true},"outputs":[],"execution_count":673},{"cell_type":"code","source":"#!g1.1\n# Sample is a dict of utt, word and label\ntrain_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\nval_set = SpeechCommandDataset(csv=val_df)","metadata":{"id":"PDPLht5fqUYe","cellId":"xtu6hfn49t7s9ikxaywmv","trusted":true},"outputs":[],"execution_count":674},{"cell_type":"markdown","source":"### Sampler for oversampling:","metadata":{"cellId":"k8odzf4cerajo3d6xn5ilh","id":"2vbPDqd6qUYj"}},{"cell_type":"code","source":"#!g1.1\n# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n\ndef get_sampler(target):\n    class_sample_count = np.array(\n        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n    weight = 1. / class_sample_count\n    samples_weight = np.array([weight[t] for t in target])\n    samples_weight = torch.from_numpy(samples_weight)\n    samples_weigth = samples_weight.float()\n    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n    return sampler","metadata":{"id":"rfnjRKo2qUYj","cellId":"qsqd9dlw6jf45vomyz1lo","trusted":true},"outputs":[],"execution_count":675},{"cell_type":"code","source":"#!g1.1\ntrain_sampler = get_sampler(train_set.csv['label'].values)","metadata":{"id":"UM8gLmHeqUYj","cellId":"8bkulr7vxvclt72pdnbyp","trusted":true},"outputs":[],"execution_count":676},{"cell_type":"code","source":"#!g1.1\nclass Collator:\n    \n    def __call__(self, data):\n        wavs = []\n        labels = []    \n\n        for el in data:\n            wavs.append(el['wav'])\n            labels.append(el['label'])\n\n        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n        wavs = pad_sequence(wavs, batch_first=True)    \n        labels = torch.Tensor(labels).long()\n        return wavs, labels","metadata":{"id":"lyBqbxp0h9pO","cellId":"3iv9e5e3kdi48n5uccz3lk","trusted":true},"outputs":[],"execution_count":677},{"cell_type":"markdown","source":"###  Dataloaders","metadata":{"cellId":"grzehw1y7a80ix9dqdj35fu","id":"e8G9xPRVqUYk"}},{"cell_type":"code","source":"#!g1.1\n# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n# num_workers=0\ntrain_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n                          shuffle=False, collate_fn=Collator(),\n                          sampler=train_sampler,\n                          num_workers=0, pin_memory=True)\n\nval_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n                        shuffle=False, collate_fn=Collator(),\n                        num_workers=0, pin_memory=True)","metadata":{"id":"6wGBMcQiqUYk","cellId":"4brhrqlbs7u9m3j2b0tf6m","trusted":true},"outputs":[],"execution_count":678},{"cell_type":"markdown","source":"### Creating MelSpecs on GPU for speeeed: ","metadata":{"cellId":"c0gjhvddd6avsp9lbw77j","id":"kTlsn6cpqUYk"}},{"cell_type":"code","source":"#!g1.1\nclass LogMelspec:\n\n    def __init__(self, is_train, config):\n        # with augmentations\n        if is_train:\n            self.melspec = nn.Sequential(\n                torchaudio.transforms.MelSpectrogram(\n                    sample_rate=config.sample_rate,\n                    n_fft=400,\n                    win_length=400,\n                    hop_length=160,\n                    n_mels=config.n_mels\n                ),\n                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n                torchaudio.transforms.TimeMasking(time_mask_param=35),\n            ).to(config.device)\n\n        # no augmentations\n        else:\n            self.melspec = torchaudio.transforms.MelSpectrogram(\n                sample_rate=config.sample_rate,\n                n_fft=400,\n                win_length=400,\n                hop_length=160,\n                n_mels=config.n_mels\n            ).to(config.device)\n\n    def __call__(self, batch):\n        # already on device\n        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))","metadata":{"id":"pRXMt6it56fW","cellId":"coteladat6tqbpzxebi57","trusted":true},"outputs":[],"execution_count":679},{"cell_type":"code","source":"#!g1.1\nmelspec_train = LogMelspec(is_train=True, config=TaskConfig)\nmelspec_val = LogMelspec(is_train=False, config=TaskConfig)","metadata":{"id":"Pqkz4_gn8BiF","cellId":"d2ngidwbc9rpqbhyg2lk6","trusted":true},"outputs":[],"execution_count":680},{"cell_type":"markdown","source":"### Quality measurment functions:","metadata":{"cellId":"kpfnv613pdp0nrrjjart","id":"zoAxmihY8yxr"}},{"cell_type":"code","source":"#!g1.1\n# FA - true: 0, model: 1\n# FR - true: 1, model: 0\n\ndef count_FA_FR(preds, labels):\n    FA = torch.sum(preds[labels == 0])\n    FR = torch.sum(labels[preds == 0])\n    \n    # torch.numel - returns total number of elements in tensor\n    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)","metadata":{"id":"euwD1UyuqUYk","cellId":"jkjd1frbnz8mojsdwlsdep","trusted":true},"outputs":[],"execution_count":681},{"cell_type":"code","source":"#!g1.1\ndef get_au_fa_fr(probs, labels):\n    sorted_probs, _ = torch.sort(probs)\n    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n    labels = torch.cat(labels, dim=0)\n        \n    FAs, FRs = [], []\n    for prob in sorted_probs:\n        preds = (probs >= prob) * 1\n        FA, FR = count_FA_FR(preds, labels)        \n        FAs.append(FA)\n        FRs.append(FR)\n    # plt.plot(FAs, FRs)\n    # plt.show()\n\n    # ~ area under curve using trapezoidal rule\n    return -np.trapz(FRs, x=FAs)","metadata":{"id":"YHBUrkT1qUYk","cellId":"3mo1elaszlvj7zifisdo0j","trusted":true},"outputs":[],"execution_count":682},{"cell_type":"markdown","source":"# Model","metadata":{"cellId":"hwz6w35ewidcjgz937p0e","id":"CcEP5cEZqUYl"}},{"cell_type":"code","source":"#!g1.1\nclass Attention(nn.Module):\n\n    def __init__(self, hidden_size: int):\n        super().__init__()\n\n        self.energy = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.Tanh(),\n            nn.Linear(hidden_size, 1)\n        )\n    \n    def forward(self, input):\n        energy = self.energy(input)\n        alpha = torch.softmax(energy, dim=-2)\n        return (input * alpha).sum(dim=-2)\n\nclass CRNN(nn.Module):\n\n    def __init__(self, config: TaskConfig):\n        super().__init__()\n        self.config = config\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1, out_channels=config.cnn_out_channels,\n                kernel_size=config.kernel_size, stride=config.stride\n            ),\n            nn.Flatten(start_dim=1, end_dim=2),\n        )\n\n        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n            config.stride[0] + 1\n        \n        self.gru = nn.GRU(\n            input_size=self.conv_out_frequency * config.cnn_out_channels,\n            hidden_size=config.hidden_size,\n            num_layers=config.gru_num_layers,\n            dropout=0.1,\n            bidirectional=config.bidirectional,\n            batch_first=True\n        )\n\n        self.attention = Attention(config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n    \n    def forward(self, input):\n        input = input.unsqueeze(dim=1)\n        conv_output = self.conv(input).transpose(-1, -2)\n        gru_output, _ = self.gru(conv_output)\n        contex_vector = self.attention(gru_output)\n        output = self.classifier(contex_vector)\n        return output\n\nconfig = TaskConfig()\nmodel = CRNN(config)\nmodel","metadata":{"outputId":"1af690bf-bba6-4033-84e7-b400d37c846b","id":"2cP_pFIsy5p2","cellId":"kj81ktfzrko63frbdskg3v","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"CRNN(\n  (conv): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n    (1): Flatten(start_dim=1, end_dim=2)\n  )\n  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n  (attention): Attention(\n    (energy): Sequential(\n      (0): Linear(in_features=64, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=1, bias=True)\n    )\n  )\n  (classifier): Linear(in_features=64, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":683},{"cell_type":"code","source":"#!g1.1\ndef train_epoch(model, opt, loader, log_melspec, device):\n    model.train()\n    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n        batch, labels = batch.to(device), labels.to(device)\n        batch = log_melspec(batch)\n\n        opt.zero_grad()\n\n        # run model # with autocast():\n        logits = model(batch)\n        # we need probabilities so we use softmax & CE separately\n        probs = F.softmax(logits, dim=-1)\n        loss = F.cross_entropy(logits, labels)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n\n        opt.step()\n\n        # logging\n        argmax_probs = torch.argmax(probs, dim=-1)\n        FA, FR = count_FA_FR(argmax_probs, labels)\n        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n\n    return acc","metadata":{"id":"DmmSFvWaqUYn","cellId":"xmmfbpgulgyxpizjyiew9","trusted":true},"outputs":[],"execution_count":684},{"cell_type":"code","source":"#!g1.1\n@torch.no_grad()\ndef validation(model, loader, log_melspec, device):\n    model.eval()\n\n    val_losses, accs, FAs, FRs = [], [], [], []\n    all_probs, all_labels = [], []\n    for i, (batch, labels) in tqdm(enumerate(loader)):\n        batch, labels = batch.to(device), labels.to(device)\n        batch = log_melspec(batch)\n\n        output = model(batch)\n        # we need probabilities so we use softmax & CE separately\n        probs = F.softmax(output, dim=-1)\n        loss = F.cross_entropy(output, labels)\n\n        # logging\n        argmax_probs = torch.argmax(probs, dim=-1)\n        all_probs.append(probs[:, 1].cpu())\n        all_labels.append(labels.cpu())\n        val_losses.append(loss.item())\n        accs.append(\n            torch.sum(argmax_probs == labels).item() /  # ???\n            torch.numel(argmax_probs)\n        )\n        FA, FR = count_FA_FR(argmax_probs, labels)\n        FAs.append(FA)\n        FRs.append(FR)\n\n    # area under FA/FR curve for whole loader\n    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n    return au_fa_fr","metadata":{"id":"UIeRbn4tqUYo","cellId":"eys29ls42yu54hv5m6cd29","trusted":true},"outputs":[],"execution_count":685},{"cell_type":"code","source":"#!g1.1\nfrom collections import defaultdict\nfrom IPython.display import clear_output\nfrom matplotlib import pyplot as plt\n\nhistory = defaultdict(list)","metadata":{"id":"PpyvKwp0k3IU","cellId":"glxy9srvahus2ysqhpnzxn","trusted":true},"outputs":[],"execution_count":686},{"cell_type":"markdown","source":"# Training","metadata":{"cellId":"379j1sflm8roxc129iltw","id":"GSNW-nZCJ4Q0"}},{"cell_type":"code","source":"config = TaskConfig()\nmodel = CRNN(config).to(config.device)\n\nprint(model)\n\nopt = torch.optim.Adam(\n    model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)","metadata":{"outputId":"f72cf20f-0877-4be9-8b6a-789086928b6c","id":"Q8sVpHNoocgA","cellId":"9doqqetedbljm1scpuvih","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":"CRNN(\n  (conv): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n    (1): Flatten(start_dim=1, end_dim=2)\n  )\n  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n  (attention): Attention(\n    (energy): Sequential(\n      (0): Linear(in_features=64, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=1, bias=True)\n    )\n  )\n  (classifier): Linear(in_features=64, out_features=2, bias=True)\n)\n"}],"execution_count":25},{"cell_type":"code","source":"sum([p.numel() for p in model.parameters()])","metadata":{"outputId":"35174419-43cb-4690-fa8a-773abca4f9cf","id":"zedXm9dmINAE","cellId":"ogslfcv9sjjmnw264v0vc","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"70443"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# TRAIN\n\nfor n in range(TaskConfig.num_epochs):\n\n    train_epoch(model, opt, train_loader,\n                melspec_train, config.device)\n\n    au_fa_fr = validation(model, val_loader,\n                          melspec_val, config.device)\n    history['val_metric'].append(au_fa_fr)\n\n    clear_output()\n    plt.plot(history['val_metric'])\n    plt.ylabel('Metric')\n    plt.xlabel('Epoch')\n    plt.grid()\n    plt.show()\n\n    print('END OF EPOCH', n)","metadata":{"outputId":"ae6ad76d-c127-4438-a2a5-7806838dcb37","id":"32oooz4lqUYo","cellId":"c7uirbuidf465hsymoq2o","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":297}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":"END OF EPOCH 19\n"}],"execution_count":27},{"cell_type":"code","source":"history","metadata":{"outputId":"94e0c5a2-d1ed-4f7b-8691-069bb2aabfd9","id":"WBkTUHZcVugz","cellId":"y4lxhictw8m6vsh13rbksf","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"defaultdict(list,\n            {'val_metric': [0.00044107337681586876,\n              0.00023113505291200655,\n              0.00017613216685680765,\n              0.00013031333652895508,\n              0.00011266133078399702,\n              7.82405163360869e-05,\n              6.526706789472828e-05,\n              6.542222413183745e-05,\n              6.129268120570122e-05,\n              4.69765345585902e-05,\n              4.318714184457791e-05,\n              4.3658578103486524e-05,\n              4.1945891947704585e-05,\n              3.883679965794018e-05,\n              4.0239173339503774e-05,\n              4.0674804312925667e-05,\n              3.106108515973857e-05,\n              2.7886349846581816e-05,\n              2.4914511151183194e-05,\n              3.3042310956670975e-05]})"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"### Reproduce","metadata":{"cellId":"ryy0mkpdrwil8u3ir87b7","id":"QRZB9KXyVvfa"}},{"cell_type":"code","source":"history = defaultdict(list)\nconfig = TaskConfig()\nmodel = CRNN(config).to(config.device)\n\nprint(model)\n\nopt = torch.optim.Adam(\n    model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)","metadata":{"outputId":"3b7e14c2-41c5-4d95-a3a3-1eb885976a4d","id":"t55FUkOGh9pT","cellId":"kx5qi79f5y6cska6qe2ar","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":"CRNN(\n  (conv): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n    (1): Flatten(start_dim=1, end_dim=2)\n  )\n  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n  (attention): Attention(\n    (energy): Sequential(\n      (0): Linear(in_features=64, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=1, bias=True)\n    )\n  )\n  (classifier): Linear(in_features=64, out_features=2, bias=True)\n)\n"}],"execution_count":29},{"cell_type":"code","source":"# TRAIN\n\nfor n in range(TaskConfig.num_epochs):\n\n    train_epoch(model, opt, train_loader,\n                melspec_train, config.device)\n\n    au_fa_fr = validation(model, val_loader,\n                          melspec_val, config.device)\n    history['val_metric'].append(au_fa_fr)\n\n    clear_output()\n    plt.plot(history['val_metric'])\n    plt.ylabel('Metric')\n    plt.xlabel('Epoch')\n    plt.grid()\n    plt.show()\n\n    print('END OF EPOCH', n)","metadata":{"outputId":"868362be-33b1-48d7-a00f-7efa3ef91777","id":"RqGOlKZbVt6j","cellId":"ocrslyue4q4qdth9vax18","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":301}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":"END OF EPOCH 19\n"}],"execution_count":30},{"cell_type":"code","source":"validation(model, val_loader, melspec_val, config.device)","metadata":{"cellId":"ckjuv33ndwv493c3t21rrs","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"102it [00:25,  4.04it/s]\n"},{"output_type":"display_data","data":{"text/plain":"2.9282755980564294e-05"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"history","metadata":{"outputId":"9e4704b3-28f2-432d-c703-91531bf95437","id":"52G9D8SDV1Qa","cellId":"amzn7gi4069agjkltmm5rj","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"defaultdict(list,\n            {'val_metric': [0.0005975096528811842,\n              0.00029343326588512686,\n              0.00021689946815724067,\n              0.00012216166653314278,\n              9.416790083010079e-05,\n              8.652347237868387e-05,\n              6.78808537352596e-05,\n              6.743328766667546e-05,\n              5.427484525030207e-05,\n              5.136864957829579e-05,\n              4.769860781590593e-05,\n              4.055545336130323e-05,\n              4.367648074622989e-05,\n              4.774634819655491e-05,\n              5.2132495668679375e-05,\n              4.695266436826571e-05,\n              4.694669682068459e-05,\n              4.4935633285846576e-05,\n              3.2887154719561814e-05,\n              2.9282755980564294e-05]})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"torch.save(model.state_dict(), 'CRNN_Model.pth')","metadata":{"cellId":"ln2xf7toerhkhj9xj4oat","trusted":true},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"# Streaming","metadata":{"cellId":"5ywwzqray7v1wswfpngie5","id":"1guSqniIfz54"}},{"cell_type":"code","source":"class Streaming(CRNN):\n    def __init__(self, config, max_window_length, streaming_step_size):\n        super().__init__(config)\n        self.max_window_length = max_window_length\n        self.streaming_step_size = streaming_step_size\n        self.buffer = []\n        self.last_hidden = torch.zeros(self.config.gru_num_layers, 1, self.config.hidden_size)\n        \n        self.num_strides_del = self.config.kernel_size[1] // self.config.stride[1] + 1\n        self.grus = []\n        self.alphas = []\n        \n    def forward(self, mel_spec): \n        \n        if len(self.buffer) * self.streaming_step_size < self.max_window_length:\n            self.buffer.append(mel_spec)\n            return None\n        \n        else:\n            \n            self.buffer.append(mel_spec)\n            to_conv = torch.cat(self.buffer, dim=1)[:, -(self.config.kernel_size[1] -1 + self.streaming_step_size):]\n            tc = to_conv.unsqueeze(0).unsqueeze(0)\n            conv_output = self.conv(tc).transpose(-1, -2)\n            gru_output, self.last_hidden = self.gru(conv_output, self.last_hidden)\n            self.grus.extend([g for g in gru_output])\n            new_alphas = self.attention.energy(gru_output)\n            self.alphas.extend([nw for nw in new_alphas])\n            c = (torch.cat(self.alphas).unsqueeze(0) * torch.cat(self.grus))\n            c2 = self.classifier((c.sum(dim=1)))\n            output = c2\n            del self.buffer[0]\n            del self.grus[self.num_strides_del:]\n            del self.alphas[self.num_strides_del:]\n            return output\n        ","metadata":{"cellId":"tbsuem05qkgokfsrcrk6oj","trusted":true},"outputs":[],"execution_count":607},{"cell_type":"code","source":"ss = 20","metadata":{"cellId":"u4vkrp8qlop5dd0rffpu9a","trusted":true},"outputs":[],"execution_count":615},{"cell_type":"code","source":"s = Streaming(config, max_window_length=40, streaming_step_size=ss)\ns.load_state_dict(torch.load('CRNN_Model.pth'))","metadata":{"cellId":"vqrx52p0gd1yqlzv9b4hl","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":616},{"cell_type":"code","source":"x = torch.cat((melspec_val(val_set[0]['wav']), melspec_val(val_set[59]['wav']), melspec_val(val_set[1]['wav'])), dim=1)\nprint(val_set[0]['label'], val_set[44]['label'], val_set[1]['label'])\nprint(val_set[0]['keywors'], val_set[44]['keywors'], val_set[1]['keywors'])","metadata":{"cellId":"s9h2f7h80lwowdwqex7tf","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"0 1 0\nthree sheila seven\n"}],"execution_count":623},{"cell_type":"code","source":"result = []\nfor i in range(0, x.shape[1], ss):\n    if i+ss < x.shape[1]:\n        output = s(x[:, i:i+ss])\n        if output == None:\n            pass\n        else:\n            result.append(F.softmax(output, dim=-1)[0][1])","metadata":{"cellId":"j611eihg1abrrjxz3xc87","trusted":true},"outputs":[],"execution_count":624},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nplt.grid()\nplt.title('Probabilities of keyword in audio')\nplt.axvline(x = melspec_val(val_set[0]['wav']).shape[-1] // ss, color = 'red', linestyle = '-')\nplt.plot(result)\nplt.axvline(x = (melspec_val(val_set[0]['wav']).shape[-1] + melspec_val(val_set[59]['wav']).shape[-1]) // ss, color = 'red', linestyle = '-')\nplt.legend(['Borders of the key-word', 'Probabilities'])","metadata":{"cellId":"1b6ddul85pgcf9zely9ro","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<matplotlib.legend.Legend at 0x7fd4601d9d50>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x504 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":626},{"cell_type":"markdown","source":"# Distillation","metadata":{"cellId":"4vttp0and5ujnmd6l1sbxb"}},{"cell_type":"markdown","source":"## Base Distillation\n","metadata":{"cellId":"l46kmhoxkipaxx04w4w1s"}},{"cell_type":"code","source":"%pip install thop","metadata":{"cellId":"64f1a99rrbmayucg81a9c","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting thop\n  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\nRequirement already satisfied: torch>=1.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from thop) (1.9.1)\nRequirement already satisfied: typing-extensions in /kernel/lib/python3.7/site-packages (from torch>=1.0.0->thop) (4.0.0)\nInstalling collected packages: thop\nSuccessfully installed thop-0.0.31.post2005241907\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"}],"execution_count":627},{"cell_type":"code","source":"from thop import profile","metadata":{"cellId":"lddydxoh1pxwx13xwr3hf","trusted":true},"outputs":[],"execution_count":628},{"cell_type":"code","source":"#!g1.1\n@dataclasses.dataclass\nclass TaskConfig:\n    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n    batch_size: int = 128\n    learning_rate: float = 3e-4\n    weight_decay: float = 1e-5\n    num_epochs: int = 20\n    n_mels: int = 40\n    cnn_out_channels: int = 8\n    kernel_size: Tuple[int, int] = (5, 20)\n    stride: Tuple[int, int] = (2, 8)\n    hidden_size: int = 64\n    gru_num_layers: int = 2\n    bidirectional: bool = False\n    num_classes: int = 2\n    sample_rate: int = 16000\n    device: torch.device = torch.device(\n        'cuda:0' if torch.cuda.is_available() else 'cpu')\n        \n\nconfig = TaskConfig()\nteacher_model = CRNN(config).to(config.device)\nteacher_model.load_state_dict(torch.load('CRNN_Model.pth'))\n\nprofile(teacher_model, (torch.randn(1, 40, 101).cuda(), ))  ","metadata":{"cellId":"ea0nkpiuwtsc59gdzemnlo","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001B[00m\n"},{"output_type":"display_data","data":{"text/plain":"(933808.0, 70443.0)"},"metadata":{}}],"execution_count":631},{"cell_type":"code","source":"#!g1.1\nfrom thop import clever_format\nmacs, params = profile(teacher_model, (torch.randn(1, 40, 101).cuda(), ))\nmacs, params = clever_format([macs, params], \"%.3f\")","metadata":{"cellId":"oae7v9pvlmpxv58mvpq5","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001B[00m\n"}],"execution_count":632},{"cell_type":"code","source":"print(macs, params)","metadata":{"cellId":"m21jv2oxdpdps20b1byv2s","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"933.808K 70.443K\n"}],"execution_count":711},{"cell_type":"code","source":"#!g1.1\n\ndef train_epoch(model, student_model, opt, loader, log_melspec, device, alpha=0.1):\n    # alpha -    \n    model.eval()\n    student_model.train()\n    \n    mse_losses = []\n    cres = []\n    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n        \n        batch, labels = batch.to(device), labels.to(device)\n        batch = log_melspec(batch)\n\n        opt.zero_grad()\n        # run model # with autocast():\n        logits_teacher = model(batch).detach()\n        logits_student = student_model(batch)\n        \n        probs = F.softmax(logits_student, dim=-1)\n        mse = F.mse_loss(logits_student, logits_teacher)\n        cre = F.cross_entropy(logits_student, labels)\n        loss = (mse * alpha) + (cre * (1. - alpha))\n        \n        ###\n        mse_losses.append(mse.detach().cpu().numpy())\n        cres.append(cre.detach().cpu().numpy())\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n\n        opt.step()\n\n        # logging\n        argmax_probs = torch.argmax(probs, dim=-1)\n        FA, FR = count_FA_FR(argmax_probs, labels)\n        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n        ####\n\n\n    return acc, np.mean(mse_losses), np.mean(cres)","metadata":{"cellId":"nuw4dy732j1tquxh7zxsy","trusted":true},"outputs":[],"execution_count":744},{"cell_type":"code","source":"#!g1.1\n@dataclasses.dataclass\nclass TaskConfig:\n    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n    batch_size: int = 128\n    learning_rate: float = 3e-4\n    weight_decay: float = 1e-5\n    num_epochs: int = 20\n    n_mels: int = 40\n    cnn_out_channels: int = 8\n    kernel_size: Tuple[int, int] = (5, 20)\n    stride: Tuple[int, int] = (2, 8)\n    hidden_size: int = 64\n    gru_num_layers: int = 2\n    bidirectional: bool = False\n    num_classes: int = 2\n    sample_rate: int = 16000\n    device: torch.device = torch.device(\n        'cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"cellId":"azhd6lo24wtmmz13yfnh8","trusted":true},"outputs":[],"execution_count":745},{"cell_type":"code","source":"#!g1.1\nsum([param.numel() for param in student_model.parameters()])","metadata":{"cellId":"fh6dtk4i146qbwa1n0b3tt","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"9047"},"metadata":{}}],"execution_count":746},{"cell_type":"code","source":"#!g1.1\nmacs, params = profile(student_model, (torch.randn(1, 40, 101).cuda(), ))\nmacs, params = clever_format([macs, params], \"%.3f\")","metadata":{"cellId":"bo062wd2c0puns2nj7907","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001B[00m\n"}],"execution_count":747},{"cell_type":"code","source":"#!g1.1\nteacher_metrics = history['val_metric']","metadata":{"cellId":"hp7ttqn6nyuylgr5nvyf5","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"[0.0005975096528811842,\n 0.00029343326588512686,\n 0.00021689946815724067,\n 0.00012216166653314278,\n 9.416790083010079e-05,\n 8.652347237868387e-05,\n 6.78808537352596e-05,\n 6.743328766667546e-05,\n 5.427484525030207e-05,\n 5.136864957829579e-05,\n 4.769860781590593e-05,\n 4.055545336130323e-05,\n 4.367648074622989e-05,\n 4.774634819655491e-05,\n 5.2132495668679375e-05,\n 4.695266436826571e-05,\n 4.694669682068459e-05,\n 4.4935633285846576e-05,\n 3.2887154719561814e-05,\n 2.9282755980564294e-05]"},"metadata":{}}],"execution_count":639},{"cell_type":"code","source":"#!g1.1\nconfig = TaskConfig()\nteacher_model = CRNN(config).to(config.device)\nteacher_model.load_state_dict(torch.load('CRNN_Model.pth'))","metadata":{"cellId":"18imjs9jbvwwe1axc2xe2b","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":748},{"cell_type":"code","source":"#!g1.1\nprofile(student_model, (torch.randn(1, 40, 101).cuda(), )) ","metadata":{"cellId":"10xlapcodxg8h7owxttaar","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001B[00m\n"},{"output_type":"stream","name":"stderr","text":"/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n"},{"output_type":"display_data","data":{"text/plain":"(177492.0, 9047.0)"},"metadata":{}}],"execution_count":764},{"cell_type":"code","source":"#!g1.1\nconfig = TaskConfig()\nteacher_model = CRNN(config).to(config.device)\nteacher_model.load_state_dict(torch.load('CRNN_Model.pth'))\n\nonfig = TaskConfig()\nconfig.cnn_out_channels = 4\nconfig.hidden_size = 20\nstudent_model = CRNN(config).to(config.device)\n\n\nopt = torch.optim.Adam(\n    student_model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\nmelspec_train = LogMelspec(is_train=True, config=TaskConfig)\nmelspec_val = LogMelspec(is_train=False, config=TaskConfig)","metadata":{"cellId":"8nr6n70fyki5tmevybthax","trusted":true},"outputs":[],"execution_count":753},{"cell_type":"code","source":"#!g1.1\nstudent_metrics3 = []\naccs_train = []\ncres_train = []\nmses_train = []\ntrain_ces = []\n\n\nfor n in range(TaskConfig.num_epochs):\n\n    accs, mse_losses, cres = train_epoch(teacher_model, student_model, opt, train_loader, melspec_train, config.device, alpha=0.5)\n\n    au_fa_fr = validation(student_model, val_loader,\n                          melspec_val, config.device)\n    student_metrics3.append(au_fa_fr)\n    mses_train.append(mse_losses)\n    cres_train.append(cres)\n    accs_train.append(accs)\n    print(accs)\n    print(au_fa_fr)","metadata":{"cellId":"x876roaptsfjdtecmj897","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"100%|| 405/405 [01:11<00:00,  5.69it/s]\n102it [00:14,  6.86it/s]\n100%|| 405/405 [01:11<00:00,  5.64it/s]\n102it [00:14,  6.92it/s]\n100%|| 405/405 [01:10<00:00,  5.73it/s]\n102it [00:11,  8.57it/s]\n100%|| 405/405 [01:12<00:00,  5.62it/s]\n102it [00:14,  6.95it/s]\n100%|| 405/405 [01:10<00:00,  5.73it/s]\n102it [00:11,  8.59it/s]\n100%|| 405/405 [01:12<00:00,  5.59it/s]\n102it [00:14,  6.90it/s]\n100%|| 405/405 [01:10<00:00,  5.71it/s]\n102it [00:11,  8.66it/s]\n100%|| 405/405 [01:12<00:00,  5.60it/s]\n102it [00:14,  6.93it/s]\n100%|| 405/405 [01:10<00:00,  5.77it/s]\n102it [00:11,  8.64it/s]\n100%|| 405/405 [01:12<00:00,  5.61it/s]\n102it [00:14,  6.96it/s]\n100%|| 405/405 [01:10<00:00,  5.73it/s]\n102it [00:11,  8.60it/s]\n100%|| 405/405 [01:12<00:00,  5.62it/s]\n102it [00:15,  6.54it/s]\n100%|| 405/405 [01:10<00:00,  5.72it/s]\n102it [00:12,  8.47it/s]\n100%|| 405/405 [01:11<00:00,  5.67it/s]\n102it [00:15,  6.71it/s]\n100%|| 405/405 [01:10<00:00,  5.72it/s]\n102it [00:12,  8.01it/s]\n100%|| 405/405 [01:11<00:00,  5.66it/s]\n102it [00:14,  7.21it/s]\n100%|| 405/405 [01:11<00:00,  5.65it/s]\n102it [00:15,  6.76it/s]\n100%|| 405/405 [01:13<00:00,  5.50it/s]\n102it [00:15,  6.44it/s]\n100%|| 405/405 [01:13<00:00,  5.51it/s]\n102it [00:13,  7.31it/s]\n100%|| 405/405 [01:13<00:00,  5.51it/s]\n102it [00:16,  6.02it/s]\n"},{"output_type":"stream","name":"stdout","text":"tensor(0.6719, device='cuda:0')\n0.001175970893883428\ntensor(0.9531, device='cuda:0')\n0.0008208302022357118\ntensor(0.8594, device='cuda:0')\n0.0007051105033163899\ntensor(0.5938, device='cuda:0')\n0.0005576106297538043\ntensor(0.8281, device='cuda:0')\n0.0004402588065710457\ntensor(0.8125, device='cuda:0')\n0.0003376945662943066\ntensor(0.7500, device='cuda:0')\n0.00029434928443882895\ntensor(0.7656, device='cuda:0')\n0.00027234096895965203\ntensor(0.8750, device='cuda:0')\n0.0002518603456612423\ntensor(0.6875, device='cuda:0')\n0.00021191059837942294\ntensor(0.8125, device='cuda:0')\n0.0002072648625875197\ntensor(0.6406, device='cuda:0')\n0.0001903707353853641\ntensor(0.6875, device='cuda:0')\n0.0001623053591113486\ntensor(0.8281, device='cuda:0')\n0.00014798324491665648\ntensor(0.8281, device='cuda:0')\n0.0001578834063537374\ntensor(0.9531, device='cuda:0')\n0.00014880975025664182\ntensor(0.9062, device='cuda:0')\n0.00012428611347202213\ntensor(0.9219, device='cuda:0')\n0.00010597767749314068\ntensor(0.9531, device='cuda:0')\n0.00010952240075632697\ntensor(0.9375, device='cuda:0')\n8.807503474977551e-05\n"}],"execution_count":754},{"cell_type":"code","source":"#!g1.1\nprint(student_metrics3[-1])","metadata":{"cellId":"acipp8m0lb8is3u06slql","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"8.807503474977551e-05\n"}],"execution_count":759},{"cell_type":"code","source":"#!g1.1\ntorch.save(student_model.state_dict(), 'student1.pth')","metadata":{"cellId":"719saoro3xckfcmbq0h20e","trusted":true},"outputs":[],"execution_count":761},{"cell_type":"code","source":"plt.plot(teacher_metrics)\nplt.plot(student_metrics3)\nplt.legend(['Teacher', 'Student distill'])\nplt.ylabel('Metric')\nplt.xlabel('Epoch')\nplt.grid()\nplt.show()","metadata":{"cellId":"gix0y4lvl4wfkqfpv35u","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":763},{"cell_type":"code","source":"#!g1.1\nconfig = TaskConfig()\nteacher = CRNN(config).to(config.device)\nteacher.load_state_dict(torch.load('CRNN_Model.pth'))\n\nconfig = TaskConfig()\nconfig.cnn_out_channels = 3\nconfig.kernel_size = (3, 20)\nconfig.hidden_size = 18\nconfig.gru_num_layers = 2\n\n\nstudent_model = CRNN(config).to(config.device)\nprofile(student_model, (torch.randn(1, 40, 101).cuda(), )) \n\nopt = torch.optim.Adam(\n    student_model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\n","metadata":{"cellId":"tmekca1xrya8rym9ugkje","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001B[00m\n"}],"execution_count":808},{"cell_type":"code","source":"#!g1.1\nsum([param.numel() for param in student_model.parameters()])","metadata":{"cellId":"hbuqge7kg8muqgt1fzj9r","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"6792"},"metadata":{}}],"execution_count":817},{"cell_type":"code","source":"#!g1.1\nstudent_metrics32 = []\naccs_train = []\ncres_train = []\nmses_train = []\ntrain_ces = []\n\n\nfor n in range(TaskConfig.num_epochs):\n\n    accs, mse_losses, cres = train_epoch(teacher_model, student_model, opt, train_loader, melspec_train, config.device, alpha=0.5)\n\n    au_fa_fr = validation(student_model, val_loader,\n                          melspec_val, config.device)\n    student_metrics32.append(au_fa_fr)\n    mses_train.append(mse_losses)\n    cres_train.append(cres)\n    accs_train.append(accs)\n    print(accs)\n    print(au_fa_fr)","metadata":{"cellId":"1ysifz2wca8p9vbppbrkj","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":" self.batch_first)\n 55%|    | 222/405 [00:51<00:39,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 223/405 [00:51<00:40,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 224/405 [00:51<00:39,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 225/405 [00:52<00:40,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 226/405 [00:52<00:40,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 227/405 [00:52<00:39,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 228/405 [00:52<00:39,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 229/405 [00:53<00:39,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 230/405 [00:53<00:39,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 231/405 [00:53<00:39,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 232/405 [00:53<00:38,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 233/405 [00:53<00:38,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 234/405 [00:54<00:37,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 235/405 [00:54<00:36,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 236/405 [00:54<00:36,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 237/405 [00:54<00:37,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 238/405 [00:55<00:37,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 239/405 [00:55<00:36,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 240/405 [00:55<00:36,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 241/405 [00:55<00:36,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 242/405 [00:55<00:36,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 243/405 [00:56<00:36,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 244/405 [00:56<00:36,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 245/405 [00:56<00:35,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 246/405 [00:56<00:35,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 247/405 [00:57<00:35,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 248/405 [00:57<00:35,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|   | 249/405 [00:57<00:34,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 250/405 [00:57<00:34,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 251/405 [00:57<00:33,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 252/405 [00:58<00:34,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 253/405 [00:58<00:34,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 254/405 [00:58<00:34,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 255/405 [00:58<00:34,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 256/405 [00:59<00:33,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 257/405 [00:59<00:32,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 258/405 [00:59<00:32,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 259/405 [00:59<00:33,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 260/405 [01:01<01:44,  1.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 261/405 [01:01<01:21,  1.76it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 262/405 [01:02<01:05,  2.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 263/405 [01:02<00:55,  2.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 264/405 [01:02<00:49,  2.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 265/405 [01:02<00:43,  3.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 266/405 [01:02<00:39,  3.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 267/405 [01:03<00:39,  3.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 268/405 [01:03<00:36,  3.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 269/405 [01:03<00:34,  3.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 270/405 [01:03<00:33,  4.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 271/405 [01:04<00:32,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 272/405 [01:04<00:31,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 273/405 [01:04<00:30,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 274/405 [01:04<00:29,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 275/405 [01:05<00:29,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 276/405 [01:05<00:28,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 277/405 [01:05<00:28,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 278/405 [01:05<00:27,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 279/405 [01:05<00:27,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 280/405 [01:06<00:27,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 281/405 [01:06<00:26,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 282/405 [01:06<00:26,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 283/405 [01:06<00:26,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 284/405 [01:07<00:26,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 285/405 [01:07<00:27,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 286/405 [01:07<00:27,  4.33it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 287/405 [01:07<00:26,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 288/405 [01:07<00:26,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|  | 289/405 [01:08<00:25,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 290/405 [01:08<00:25,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 291/405 [01:08<00:24,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 292/405 [01:08<00:24,  4.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 293/405 [01:08<00:24,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 294/405 [01:09<00:24,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 295/405 [01:09<00:23,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 296/405 [01:09<00:22,  4.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 297/405 [01:09<00:22,  4.77it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 298/405 [01:10<00:22,  4.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 299/405 [01:10<00:22,  4.77it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 300/405 [01:10<00:22,  4.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 301/405 [01:10<00:22,  4.67it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 302/405 [01:10<00:21,  4.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 303/405 [01:11<00:21,  4.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 304/405 [01:11<00:21,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 305/405 [01:11<00:22,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 306/405 [01:11<00:22,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 307/405 [01:12<00:22,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 308/405 [01:12<00:21,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 309/405 [01:12<00:21,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 310/405 [01:12<00:21,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 311/405 [01:12<00:21,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 312/405 [01:13<00:21,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 313/405 [01:13<00:20,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 314/405 [01:13<00:20,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 315/405 [01:13<00:21,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 316/405 [01:14<00:20,  4.29it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 317/405 [01:14<00:19,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 318/405 [01:14<00:19,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 319/405 [01:14<00:19,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 320/405 [01:15<00:19,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 321/405 [01:15<00:19,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 322/405 [01:15<00:19,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 323/405 [01:15<00:18,  4.33it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 324/405 [01:15<00:18,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 325/405 [01:16<00:18,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 326/405 [01:16<00:17,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 327/405 [01:16<00:18,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 328/405 [01:16<00:17,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 329/405 [01:17<00:18,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%| | 330/405 [01:17<00:17,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 331/405 [01:17<00:18,  4.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 332/405 [01:17<00:17,  4.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 333/405 [01:18<00:17,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 334/405 [01:18<00:16,  4.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 335/405 [01:18<00:16,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 336/405 [01:18<00:16,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 337/405 [01:19<00:16,  4.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 338/405 [01:19<00:15,  4.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 339/405 [01:19<00:15,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 340/405 [01:19<00:15,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 341/405 [01:19<00:14,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 342/405 [01:20<00:14,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 343/405 [01:20<00:13,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 344/405 [01:20<00:13,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 345/405 [01:20<00:13,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 346/405 [01:21<00:13,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 347/405 [01:21<00:13,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 348/405 [01:21<00:13,  4.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 349/405 [01:21<00:13,  4.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 350/405 [01:22<00:13,  4.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 351/405 [01:22<00:12,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 352/405 [01:22<00:12,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 353/405 [01:22<00:11,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 354/405 [01:22<00:11,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 355/405 [01:23<00:11,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 356/405 [01:23<00:11,  4.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 357/405 [01:23<00:11,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 358/405 [01:23<00:10,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 359/405 [01:24<00:10,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 360/405 [01:24<00:10,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 361/405 [01:24<00:09,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 362/405 [01:24<00:09,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 363/405 [01:25<00:09,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 364/405 [01:25<00:09,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 365/405 [01:25<00:09,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 366/405 [01:25<00:08,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 367/405 [01:25<00:08,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 368/405 [01:26<00:08,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 369/405 [01:26<00:08,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%|| 370/405 [01:26<00:08,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 371/405 [01:26<00:07,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 372/405 [01:27<00:07,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 373/405 [01:27<00:07,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 374/405 [01:27<00:06,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 375/405 [01:27<00:06,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 376/405 [01:27<00:06,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 377/405 [01:28<00:06,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 378/405 [01:28<00:06,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 379/405 [01:28<00:05,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 380/405 [01:28<00:05,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 381/405 [01:29<00:05,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 382/405 [01:29<00:05,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 383/405 [01:29<00:04,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 384/405 [01:29<00:04,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 385/405 [01:29<00:04,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 386/405 [01:30<00:04,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 387/405 [01:30<00:03,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 388/405 [01:30<00:03,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 389/405 [01:30<00:03,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 390/405 [01:31<00:03,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 391/405 [01:31<00:03,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 392/405 [01:31<00:02,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 393/405 [01:31<00:02,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 394/405 [01:31<00:02,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 395/405 [01:32<00:02,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 396/405 [01:32<00:02,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 397/405 [01:32<00:01,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 398/405 [01:32<00:01,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 399/405 [01:33<00:01,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 400/405 [01:33<00:01,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 401/405 [01:33<00:00,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 402/405 [01:33<00:00,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 403/405 [01:34<00:00,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 404/405 [01:34<00:00,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 405/405 [01:34<00:00,  4.29it/s]\n102it [00:23,  4.34it/s]\n  0%|          | 0/405 [00:00<?, ?it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  0%|          | 1/405 [00:00<01:50,  3.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  0%|          | 2/405 [00:00<01:47,  3.75it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 3/405 [00:00<01:48,  3.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 4/405 [00:01<01:44,  3.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 5/405 [00:01<01:41,  3.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|         | 6/405 [00:01<01:40,  3.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 7/405 [00:01<01:39,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 8/405 [00:02<01:39,  3.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 9/405 [00:02<01:37,  4.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 10/405 [00:02<01:35,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 11/405 [00:02<01:35,  4.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 12/405 [00:02<01:34,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 13/405 [00:03<01:35,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 14/405 [00:03<01:33,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 15/405 [00:03<01:36,  4.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 16/405 [00:03<01:37,  4.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 17/405 [00:04<01:33,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 18/405 [00:04<01:31,  4.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 19/405 [00:04<01:30,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 20/405 [00:04<01:29,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 21/405 [00:05<01:28,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 22/405 [00:05<01:27,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 23/405 [00:05<01:27,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 24/405 [00:05<01:27,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 25/405 [00:06<01:27,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 26/405 [00:06<01:27,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 27/405 [00:06<01:28,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 28/405 [00:06<01:27,  4.29it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 29/405 [00:06<01:25,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 30/405 [00:07<01:26,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 31/405 [00:07<01:28,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 32/405 [00:07<01:30,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 33/405 [00:07<01:30,  4.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 34/405 [00:08<01:32,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 35/405 [00:08<01:36,  3.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 36/405 [00:08<01:37,  3.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 37/405 [00:09<01:40,  3.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 38/405 [00:09<01:42,  3.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 39/405 [00:09<01:40,  3.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 40/405 [00:09<01:41,  3.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 41/405 [00:10<01:46,  3.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 42/405 [00:10<01:44,  3.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 43/405 [00:10<01:46,  3.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 44/405 [00:11<01:47,  3.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 45/405 [00:11<01:48,  3.33it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|        | 46/405 [00:11<01:46,  3.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 47/405 [00:11<01:46,  3.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 48/405 [00:12<01:46,  3.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 49/405 [00:12<01:48,  3.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 50/405 [00:12<01:44,  3.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 51/405 [00:13<01:46,  3.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 52/405 [00:13<01:45,  3.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 53/405 [00:13<01:42,  3.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 54/405 [00:14<01:44,  3.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 55/405 [00:14<01:42,  3.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 56/405 [00:14<01:42,  3.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 57/405 [00:14<01:41,  3.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 58/405 [00:15<01:39,  3.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 59/405 [00:15<01:33,  3.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 60/405 [00:15<01:32,  3.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 61/405 [00:16<01:35,  3.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 62/405 [00:16<01:36,  3.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 63/405 [00:16<01:43,  3.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 64/405 [00:16<01:36,  3.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 65/405 [00:17<01:35,  3.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 66/405 [00:17<01:35,  3.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 67/405 [00:17<01:33,  3.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 68/405 [00:17<01:28,  3.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 69/405 [00:18<01:24,  3.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 70/405 [00:18<01:21,  4.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 71/405 [00:18<01:23,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 72/405 [00:18<01:22,  4.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 73/405 [00:19<01:21,  4.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 74/405 [00:19<01:19,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 75/405 [00:19<01:17,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 76/405 [00:19<01:18,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 77/405 [00:20<01:20,  4.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 78/405 [00:20<01:23,  3.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 79/405 [00:20<01:20,  4.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 80/405 [00:20<01:18,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 81/405 [00:21<01:15,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 82/405 [00:21<01:13,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 83/405 [00:21<01:11,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 84/405 [00:21<01:10,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 85/405 [00:21<01:10,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 86/405 [00:22<01:09,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|       | 87/405 [00:22<01:08,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 88/405 [00:22<01:08,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 89/405 [00:22<01:09,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 90/405 [00:23<01:09,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 91/405 [00:23<01:09,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 92/405 [00:23<01:10,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 93/405 [00:23<01:08,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 94/405 [00:23<01:09,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 95/405 [00:24<01:09,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 96/405 [00:24<01:09,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 97/405 [00:24<01:10,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 98/405 [00:24<01:12,  4.26it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 99/405 [00:25<01:11,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 100/405 [00:25<01:11,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 101/405 [00:25<01:13,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 102/405 [00:25<01:14,  4.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 103/405 [00:26<01:11,  4.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 104/405 [00:26<01:13,  4.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 105/405 [00:26<01:11,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 106/405 [00:26<01:12,  4.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 107/405 [00:27<01:12,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 108/405 [00:27<01:13,  4.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 109/405 [00:27<01:14,  3.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 110/405 [00:27<01:14,  3.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 111/405 [00:28<01:11,  4.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 112/405 [00:28<01:09,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 113/405 [00:28<01:09,  4.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 114/405 [00:28<01:09,  4.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 115/405 [00:28<01:09,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 116/405 [00:29<01:09,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 117/405 [00:29<01:11,  4.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 118/405 [00:29<01:10,  4.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 119/405 [00:29<01:13,  3.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 120/405 [00:30<01:11,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 121/405 [00:30<01:10,  4.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 122/405 [00:30<01:11,  3.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 123/405 [00:31<01:13,  3.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 124/405 [00:31<01:09,  4.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 125/405 [00:31<01:10,  3.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 126/405 [00:31<01:09,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|      | 127/405 [00:31<01:08,  4.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 128/405 [00:32<01:06,  4.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 129/405 [00:32<01:05,  4.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 130/405 [00:32<01:05,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 131/405 [00:32<01:04,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 132/405 [00:33<01:04,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 133/405 [00:33<01:04,  4.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 134/405 [00:33<01:04,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 135/405 [00:33<01:03,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 136/405 [00:34<01:01,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 137/405 [00:34<00:59,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 138/405 [00:34<00:58,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 139/405 [00:34<00:59,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 140/405 [00:34<00:58,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 141/405 [00:35<00:57,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 142/405 [00:35<00:57,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 143/405 [00:35<00:56,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 144/405 [00:35<00:56,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 145/405 [00:36<00:56,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 146/405 [00:36<00:57,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 147/405 [00:36<00:57,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 148/405 [00:36<00:57,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 149/405 [00:36<00:55,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 150/405 [00:37<00:55,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 151/405 [00:37<00:55,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 152/405 [00:37<00:55,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 153/405 [00:37<00:54,  4.64it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 154/405 [00:37<00:53,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 155/405 [00:38<00:54,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 156/405 [00:38<00:55,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 157/405 [00:38<00:54,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 158/405 [00:38<00:54,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 159/405 [00:39<00:55,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 160/405 [00:39<00:54,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 161/405 [00:39<00:55,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 162/405 [00:39<00:54,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 163/405 [00:40<00:54,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 164/405 [00:40<00:55,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 165/405 [00:40<00:54,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 166/405 [00:40<00:54,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 167/405 [00:40<00:52,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|     | 168/405 [00:41<00:51,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 169/405 [00:41<00:52,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 170/405 [00:41<00:51,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 171/405 [00:41<00:50,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 172/405 [00:42<00:51,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 173/405 [00:42<00:51,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 174/405 [00:42<00:51,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 175/405 [00:42<00:51,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 176/405 [00:42<00:51,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 177/405 [00:43<00:50,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 178/405 [00:43<00:49,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 179/405 [00:43<00:49,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 180/405 [00:43<00:49,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 181/405 [00:43<00:48,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 182/405 [00:44<00:48,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 183/405 [00:44<00:48,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 184/405 [00:44<00:48,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 185/405 [00:44<00:48,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 186/405 [00:45<00:48,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 187/405 [00:45<00:47,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 188/405 [00:45<00:47,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 189/405 [00:45<00:47,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 190/405 [00:45<00:46,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 191/405 [00:46<00:46,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 192/405 [00:46<00:46,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 193/405 [00:46<00:46,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 194/405 [00:46<00:47,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 195/405 [00:47<00:47,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 196/405 [00:47<00:47,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 197/405 [00:47<00:48,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 198/405 [00:47<00:48,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 199/405 [00:48<00:47,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 200/405 [00:48<00:47,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 201/405 [00:48<00:47,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 202/405 [00:48<00:47,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 203/405 [00:48<00:48,  4.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 204/405 [00:49<00:47,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 205/405 [00:49<00:46,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 206/405 [00:49<00:48,  4.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 207/405 [00:49<00:48,  4.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|    | 208/405 [00:50<00:47,  4.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 209/405 [00:50<00:46,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 210/405 [00:50<00:45,  4.26it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 211/405 [00:50<00:46,  4.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 212/405 [00:51<00:44,  4.29it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 213/405 [00:51<00:44,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 214/405 [00:51<00:45,  4.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 215/405 [00:51<00:46,  4.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 216/405 [00:52<00:46,  4.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 217/405 [00:52<00:44,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 218/405 [00:52<00:44,  4.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 219/405 [00:52<00:45,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 220/405 [00:53<00:45,  4.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 221/405 [00:53<00:43,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 222/405 [00:53<00:42,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 223/405 [00:53<00:41,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 224/405 [00:53<00:40,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 225/405 [00:54<00:40,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 226/405 [00:54<00:41,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 227/405 [00:54<00:41,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 228/405 [00:54<00:41,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 229/405 [00:55<00:41,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 230/405 [00:55<00:40,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 231/405 [00:55<00:41,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 232/405 [00:55<00:41,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 233/405 [00:56<00:41,  4.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 234/405 [00:56<00:40,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 235/405 [00:56<00:40,  4.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 236/405 [00:56<00:39,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 237/405 [00:56<00:38,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 238/405 [00:57<00:37,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 239/405 [00:59<02:02,  1.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 240/405 [00:59<01:37,  1.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 241/405 [00:59<01:18,  2.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 242/405 [00:59<01:05,  2.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 243/405 [01:00<00:56,  2.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 244/405 [01:00<00:50,  3.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 245/405 [01:00<00:46,  3.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 246/405 [01:00<00:43,  3.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 247/405 [01:01<00:42,  3.72it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 248/405 [01:01<00:40,  3.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|   | 249/405 [01:01<00:40,  3.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 250/405 [01:01<00:38,  4.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 251/405 [01:01<00:37,  4.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 252/405 [01:02<00:37,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 253/405 [01:02<00:35,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 254/405 [01:02<00:34,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 255/405 [01:02<00:33,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 256/405 [01:03<00:32,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 257/405 [01:03<00:32,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 258/405 [01:03<00:31,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 259/405 [01:03<00:31,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 260/405 [01:03<00:32,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 261/405 [01:04<00:33,  4.26it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 262/405 [01:04<00:34,  4.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 263/405 [01:04<00:34,  4.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 264/405 [01:04<00:34,  4.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 265/405 [01:05<00:34,  4.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 266/405 [01:05<00:33,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 267/405 [01:05<00:33,  4.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 268/405 [01:05<00:32,  4.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 269/405 [01:06<00:32,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 270/405 [01:06<00:31,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 271/405 [01:06<00:31,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 272/405 [01:06<00:31,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 273/405 [01:07<00:29,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 274/405 [01:07<00:29,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 275/405 [01:07<00:29,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 276/405 [01:07<00:29,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 277/405 [01:07<00:28,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 278/405 [01:08<00:28,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 279/405 [01:08<00:29,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 280/405 [01:08<00:29,  4.26it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 281/405 [01:08<00:28,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 282/405 [01:09<00:29,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 283/405 [01:09<00:28,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 284/405 [01:09<00:28,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 285/405 [01:09<00:27,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 286/405 [01:10<00:27,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 287/405 [01:10<00:27,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 288/405 [01:10<00:27,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|  | 289/405 [01:10<00:26,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 290/405 [01:10<00:25,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 291/405 [01:11<00:25,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 292/405 [01:11<00:24,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 293/405 [01:11<00:24,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 294/405 [01:11<00:24,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 295/405 [01:12<00:24,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 296/405 [01:12<00:24,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 297/405 [01:12<00:24,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 298/405 [01:12<00:24,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 299/405 [01:12<00:23,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 300/405 [01:13<00:23,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 301/405 [01:13<00:23,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 302/405 [01:13<00:23,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 303/405 [01:13<00:23,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 304/405 [01:14<00:23,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 305/405 [01:14<00:23,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 306/405 [01:14<00:23,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 307/405 [01:14<00:22,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 308/405 [01:15<00:22,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 309/405 [01:15<00:21,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 310/405 [01:15<00:21,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 311/405 [01:15<00:22,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 312/405 [01:15<00:22,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 313/405 [01:16<00:22,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 314/405 [01:16<00:21,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 315/405 [01:16<00:21,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 316/405 [01:16<00:21,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 317/405 [01:17<00:21,  4.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 318/405 [01:17<00:20,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 319/405 [01:17<00:20,  4.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 320/405 [01:17<00:19,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 321/405 [01:18<00:19,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 322/405 [01:18<00:18,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 323/405 [01:18<00:18,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 324/405 [01:18<00:18,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 325/405 [01:18<00:17,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 326/405 [01:19<00:17,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 327/405 [01:19<00:17,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 328/405 [01:19<00:16,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 329/405 [01:19<00:16,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%| | 330/405 [01:20<00:16,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 331/405 [01:20<00:16,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 332/405 [01:20<00:15,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 333/405 [01:20<00:16,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 334/405 [01:20<00:16,  4.33it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 335/405 [01:21<00:15,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 336/405 [01:21<00:15,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 337/405 [01:21<00:15,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 338/405 [01:21<00:15,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 339/405 [01:22<00:15,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 340/405 [01:22<00:15,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 341/405 [01:22<00:15,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 342/405 [01:22<00:15,  4.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 343/405 [01:23<00:14,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 344/405 [01:23<00:13,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 345/405 [01:23<00:14,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 346/405 [01:23<00:13,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 347/405 [01:24<00:13,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 348/405 [01:24<00:13,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 349/405 [01:24<00:12,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 350/405 [01:24<00:12,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 351/405 [01:24<00:12,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 352/405 [01:25<00:11,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 353/405 [01:25<00:11,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 354/405 [01:25<00:11,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 355/405 [01:25<00:10,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 356/405 [01:25<00:10,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 357/405 [01:26<00:10,  4.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 358/405 [01:26<00:10,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 359/405 [01:26<00:10,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 360/405 [01:26<00:10,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 361/405 [01:27<00:09,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 362/405 [01:27<00:09,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 363/405 [01:27<00:09,  4.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 364/405 [01:27<00:09,  4.29it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 365/405 [01:28<00:08,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 366/405 [01:28<00:08,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 367/405 [01:28<00:08,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 368/405 [01:28<00:08,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 369/405 [01:28<00:07,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%|| 370/405 [01:29<00:07,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 371/405 [01:29<00:07,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 372/405 [01:29<00:07,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 373/405 [01:29<00:06,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 374/405 [01:29<00:06,  4.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 375/405 [01:30<00:06,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 376/405 [01:30<00:06,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 377/405 [01:30<00:06,  4.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 378/405 [01:30<00:05,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 379/405 [01:31<00:05,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 380/405 [01:31<00:05,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 381/405 [01:31<00:05,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 382/405 [01:31<00:05,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 383/405 [01:31<00:04,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 384/405 [01:32<00:04,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 385/405 [01:32<00:04,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 386/405 [01:32<00:04,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 387/405 [01:32<00:04,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 388/405 [01:33<00:03,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 389/405 [01:33<00:03,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 390/405 [01:33<00:03,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 391/405 [01:33<00:03,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 392/405 [01:34<00:02,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 393/405 [01:34<00:02,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 394/405 [01:34<00:02,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 395/405 [01:34<00:02,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 396/405 [01:34<00:01,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 397/405 [01:35<00:01,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 398/405 [01:35<00:01,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 399/405 [01:35<00:01,  4.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 400/405 [01:35<00:01,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 401/405 [01:35<00:00,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 402/405 [01:36<00:00,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 403/405 [01:36<00:00,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 404/405 [01:36<00:00,  4.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 405/405 [01:36<00:00,  4.19it/s]\n102it [00:23,  4.37it/s]\n  0%|          | 0/405 [00:00<?, ?it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  0%|          | 1/405 [00:00<01:45,  3.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  0%|          | 2/405 [00:00<01:42,  3.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 3/405 [00:00<01:46,  3.79it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 4/405 [00:01<01:44,  3.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 5/405 [00:01<01:43,  3.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|         | 6/405 [00:01<01:44,  3.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 7/405 [00:01<01:42,  3.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 8/405 [00:02<01:39,  4.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 9/405 [00:02<01:37,  4.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 10/405 [00:02<01:35,  4.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 11/405 [00:02<01:34,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 12/405 [00:02<01:35,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 13/405 [00:03<01:37,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 14/405 [00:03<01:37,  4.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 15/405 [00:03<01:36,  4.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 16/405 [00:04<01:36,  4.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 17/405 [00:04<01:34,  4.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 18/405 [00:04<01:33,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 19/405 [00:04<01:30,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 20/405 [00:04<01:29,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 21/405 [00:05<01:28,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 22/405 [00:05<01:27,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 23/405 [00:05<01:28,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 24/405 [00:05<01:28,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 25/405 [00:06<01:30,  4.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 26/405 [00:06<01:30,  4.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 27/405 [00:06<01:30,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 28/405 [00:06<01:31,  4.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 29/405 [00:07<01:35,  3.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 30/405 [00:07<01:31,  4.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 31/405 [00:07<01:29,  4.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 32/405 [00:07<01:27,  4.26it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 33/405 [00:08<01:28,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 34/405 [00:08<01:26,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 35/405 [00:08<01:27,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 36/405 [00:08<01:27,  4.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 37/405 [00:08<01:27,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 38/405 [00:09<01:27,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 39/405 [00:09<01:33,  3.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 40/405 [00:09<01:39,  3.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 41/405 [00:10<01:35,  3.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 42/405 [00:10<01:35,  3.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 43/405 [00:10<01:35,  3.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 44/405 [00:10<01:40,  3.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 45/405 [00:11<01:40,  3.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|        | 46/405 [00:11<01:42,  3.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 47/405 [00:11<01:35,  3.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 48/405 [00:11<01:30,  3.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 49/405 [00:12<01:28,  4.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 50/405 [00:12<01:24,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 51/405 [00:12<01:23,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 52/405 [00:12<01:21,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 53/405 [00:13<01:20,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 54/405 [00:13<01:20,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 55/405 [00:13<01:19,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 56/405 [00:13<01:24,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 57/405 [00:14<01:24,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 58/405 [00:14<01:27,  3.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 59/405 [00:14<01:28,  3.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 60/405 [00:14<01:28,  3.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 61/405 [00:15<01:29,  3.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 62/405 [00:15<01:28,  3.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 63/405 [00:15<01:26,  3.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 64/405 [00:15<01:26,  3.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 65/405 [00:16<01:27,  3.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 66/405 [00:16<01:26,  3.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 67/405 [00:16<01:26,  3.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 68/405 [00:16<01:31,  3.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 69/405 [00:17<01:34,  3.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 70/405 [00:17<01:29,  3.73it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 71/405 [00:17<01:29,  3.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 72/405 [00:17<01:25,  3.87it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 73/405 [00:18<01:25,  3.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 74/405 [00:18<01:27,  3.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 75/405 [00:18<01:28,  3.71it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 76/405 [00:19<01:31,  3.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 77/405 [00:19<01:34,  3.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 78/405 [00:19<01:31,  3.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 79/405 [00:19<01:31,  3.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 80/405 [00:20<01:29,  3.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 81/405 [00:20<01:27,  3.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 82/405 [00:20<01:27,  3.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 83/405 [00:20<01:25,  3.77it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 84/405 [00:21<01:29,  3.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 85/405 [00:21<01:31,  3.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 86/405 [00:21<01:30,  3.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|       | 87/405 [00:22<01:28,  3.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 88/405 [00:22<01:27,  3.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 89/405 [00:22<01:24,  3.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 90/405 [00:22<01:28,  3.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 91/405 [00:23<01:32,  3.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 92/405 [00:23<01:31,  3.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 93/405 [00:23<01:31,  3.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 94/405 [00:24<01:34,  3.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 95/405 [00:24<01:32,  3.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 96/405 [00:24<01:31,  3.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 97/405 [00:25<01:27,  3.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 98/405 [00:25<01:29,  3.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 99/405 [00:25<01:34,  3.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 100/405 [00:25<01:30,  3.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 101/405 [00:26<01:28,  3.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 102/405 [00:26<01:26,  3.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 103/405 [00:26<01:25,  3.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 104/405 [00:27<01:26,  3.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 105/405 [00:27<01:24,  3.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 106/405 [00:27<01:19,  3.76it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 107/405 [00:27<01:25,  3.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 108/405 [00:28<01:22,  3.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 109/405 [00:28<01:16,  3.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 110/405 [00:28<01:14,  3.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 111/405 [00:28<01:14,  3.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 112/405 [00:29<01:13,  3.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 113/405 [00:29<01:14,  3.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 114/405 [00:29<01:13,  3.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 115/405 [00:29<01:14,  3.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 116/405 [00:30<01:13,  3.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 117/405 [00:30<01:11,  4.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 118/405 [00:30<01:08,  4.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 119/405 [00:30<01:11,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 120/405 [00:31<01:16,  3.72it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 121/405 [00:31<01:15,  3.75it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 122/405 [00:31<01:14,  3.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 123/405 [00:31<01:12,  3.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 124/405 [00:32<01:08,  4.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 125/405 [00:32<01:06,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 126/405 [00:32<01:05,  4.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|      | 127/405 [00:32<01:04,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 128/405 [00:33<01:04,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 129/405 [00:33<01:03,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 130/405 [00:33<01:02,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 131/405 [00:33<01:02,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 132/405 [00:33<01:01,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 133/405 [00:34<01:00,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 134/405 [00:34<01:00,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 135/405 [00:34<00:59,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 136/405 [00:34<00:58,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 137/405 [00:35<00:58,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 138/405 [00:35<00:58,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 139/405 [00:35<00:58,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 140/405 [00:35<00:59,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 141/405 [00:35<00:59,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 142/405 [00:36<01:00,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 143/405 [00:36<00:59,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 144/405 [00:36<00:58,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 145/405 [00:36<00:59,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 146/405 [00:37<00:57,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 147/405 [00:37<00:59,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 148/405 [00:37<00:59,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 149/405 [00:37<00:57,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 150/405 [00:38<00:58,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 151/405 [00:38<00:58,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 152/405 [00:38<00:58,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 153/405 [00:38<00:58,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 154/405 [00:38<00:57,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 155/405 [00:39<00:57,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 156/405 [00:39<01:00,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 157/405 [00:39<01:00,  4.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 158/405 [00:39<00:59,  4.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 159/405 [00:40<00:58,  4.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 160/405 [00:40<00:55,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 161/405 [00:40<00:56,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 162/405 [00:40<00:54,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 163/405 [00:41<00:53,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 164/405 [00:41<00:53,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 165/405 [00:41<00:52,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 166/405 [00:41<00:52,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 167/405 [00:41<00:52,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|     | 168/405 [00:42<00:53,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 169/405 [00:42<00:53,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 170/405 [00:42<00:53,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 171/405 [00:42<00:52,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 172/405 [00:43<00:53,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 173/405 [00:43<00:52,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 174/405 [00:43<00:54,  4.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 175/405 [00:43<00:54,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 176/405 [00:43<00:52,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 177/405 [00:44<00:52,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 178/405 [00:44<00:52,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 179/405 [00:44<00:51,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 180/405 [00:44<00:50,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 181/405 [00:45<00:49,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 182/405 [00:45<00:51,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 183/405 [00:45<00:51,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 184/405 [00:45<00:52,  4.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 185/405 [00:46<00:51,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 186/405 [00:46<00:49,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 187/405 [00:46<00:49,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 188/405 [00:46<00:50,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 189/405 [00:46<00:48,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 190/405 [00:47<00:48,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 191/405 [00:47<00:47,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 192/405 [00:47<00:47,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 193/405 [00:47<00:47,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 194/405 [00:48<00:48,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 195/405 [00:48<00:47,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 196/405 [00:48<00:46,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 197/405 [00:48<00:47,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 198/405 [00:48<00:47,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 199/405 [00:49<00:45,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 200/405 [00:49<00:46,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 201/405 [00:49<00:47,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 202/405 [00:49<00:47,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 203/405 [00:50<00:46,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 204/405 [00:50<00:46,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 205/405 [00:50<00:44,  4.47it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 206/405 [00:50<00:44,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 207/405 [00:51<00:44,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|    | 208/405 [00:51<00:44,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 209/405 [00:51<00:45,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 210/405 [00:51<00:44,  4.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 211/405 [00:51<00:44,  4.33it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 212/405 [00:52<00:45,  4.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 213/405 [00:52<00:44,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 214/405 [00:52<00:43,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 215/405 [00:52<00:42,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 216/405 [00:54<02:17,  1.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 217/405 [00:55<01:48,  1.73it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 218/405 [00:55<01:29,  2.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 219/405 [00:55<01:14,  2.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 220/405 [00:55<01:04,  2.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 221/405 [00:55<00:57,  3.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 222/405 [00:56<00:53,  3.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 223/405 [00:56<00:51,  3.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 224/405 [00:56<00:47,  3.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 225/405 [00:56<00:44,  4.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 226/405 [00:57<00:42,  4.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 227/405 [00:57<00:41,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 228/405 [00:57<00:40,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 229/405 [00:57<00:39,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 230/405 [00:57<00:39,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 231/405 [00:58<00:38,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 232/405 [00:58<00:38,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 233/405 [00:58<00:37,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 234/405 [00:58<00:37,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 235/405 [00:59<00:37,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 236/405 [00:59<00:37,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 237/405 [00:59<00:37,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 238/405 [00:59<00:36,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 239/405 [00:59<00:36,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 240/405 [01:00<00:36,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 241/405 [01:00<00:36,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 242/405 [01:00<00:36,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 243/405 [01:00<00:35,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 244/405 [01:01<00:35,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 245/405 [01:01<00:35,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 246/405 [01:01<00:35,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 247/405 [01:01<00:35,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 248/405 [01:01<00:35,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|   | 249/405 [01:02<00:35,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 250/405 [01:02<00:35,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 251/405 [01:02<00:34,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 252/405 [01:02<00:34,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 253/405 [01:03<00:35,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 254/405 [01:03<00:34,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 255/405 [01:03<00:33,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 256/405 [01:03<00:33,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 257/405 [01:04<00:33,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 258/405 [01:04<00:33,  4.41it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 259/405 [01:04<00:32,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 260/405 [01:04<00:32,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 261/405 [01:04<00:31,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 262/405 [01:05<00:31,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 263/405 [01:05<00:31,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 264/405 [01:05<00:30,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 265/405 [01:05<00:32,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 266/405 [01:06<00:32,  4.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 267/405 [01:06<00:31,  4.33it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 268/405 [01:06<00:31,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 269/405 [01:06<00:30,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 270/405 [01:06<00:29,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 271/405 [01:07<00:29,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 272/405 [01:07<00:28,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 273/405 [01:07<00:28,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 274/405 [01:07<00:30,  4.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 275/405 [01:08<00:29,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 276/405 [01:08<00:28,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 277/405 [01:08<00:28,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 278/405 [01:08<00:27,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 279/405 [01:08<00:28,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 280/405 [01:09<00:27,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 281/405 [01:09<00:27,  4.43it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 282/405 [01:09<00:28,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 283/405 [01:09<00:29,  4.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 284/405 [01:10<00:27,  4.38it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 285/405 [01:10<00:26,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 286/405 [01:10<00:26,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 287/405 [01:10<00:26,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 288/405 [01:10<00:25,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|  | 289/405 [01:11<00:25,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 290/405 [01:11<00:24,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 291/405 [01:11<00:24,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 292/405 [01:11<00:24,  4.67it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 293/405 [01:12<00:24,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 294/405 [01:12<00:24,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 295/405 [01:12<00:24,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 296/405 [01:12<00:24,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 297/405 [01:12<00:23,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 298/405 [01:13<00:22,  4.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 299/405 [01:13<00:22,  4.71it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 300/405 [01:13<00:23,  4.49it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 301/405 [01:13<00:23,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 302/405 [01:14<00:23,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 303/405 [01:14<00:22,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 304/405 [01:14<00:22,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 305/405 [01:14<00:21,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 306/405 [01:14<00:21,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 307/405 [01:15<00:21,  4.64it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 308/405 [01:15<00:20,  4.64it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 309/405 [01:15<00:21,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 310/405 [01:15<00:20,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 311/405 [01:15<00:21,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 312/405 [01:16<00:20,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 313/405 [01:16<00:20,  4.48it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 314/405 [01:16<00:20,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 315/405 [01:16<00:20,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 316/405 [01:17<00:20,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 317/405 [01:17<00:20,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 318/405 [01:17<00:19,  4.37it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 319/405 [01:17<00:20,  4.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 320/405 [01:18<00:20,  4.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 321/405 [01:18<00:20,  4.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 322/405 [01:18<00:20,  4.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 323/405 [01:18<00:19,  4.26it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 324/405 [01:19<00:18,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 325/405 [01:19<00:18,  4.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 326/405 [01:19<00:18,  4.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 327/405 [01:19<00:18,  4.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 328/405 [01:19<00:17,  4.30it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 329/405 [01:20<00:17,  4.40it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%| | 330/405 [01:20<00:16,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 331/405 [01:20<00:16,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 332/405 [01:20<00:16,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 333/405 [01:21<00:16,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 334/405 [01:21<00:15,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 335/405 [01:21<00:15,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 336/405 [01:21<00:15,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 337/405 [01:21<00:14,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 338/405 [01:22<00:14,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 339/405 [01:22<00:14,  4.42it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 340/405 [01:22<00:14,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 341/405 [01:22<00:14,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 342/405 [01:23<00:13,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 343/405 [01:23<00:13,  4.72it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 344/405 [01:23<00:12,  4.71it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 345/405 [01:23<00:12,  4.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 346/405 [01:23<00:12,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 347/405 [01:24<00:12,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 348/405 [01:24<00:12,  4.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 349/405 [01:24<00:12,  4.46it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 350/405 [01:24<00:12,  4.51it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 351/405 [01:24<00:11,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 352/405 [01:25<00:11,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 353/405 [01:25<00:11,  4.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 354/405 [01:25<00:10,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 355/405 [01:25<00:10,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 356/405 [01:26<00:10,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 357/405 [01:26<00:10,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 358/405 [01:26<00:10,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 359/405 [01:26<00:09,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 360/405 [01:26<00:09,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 361/405 [01:27<00:09,  4.67it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 362/405 [01:27<00:09,  4.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 363/405 [01:27<00:08,  4.71it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 364/405 [01:27<00:08,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 365/405 [01:28<00:08,  4.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 366/405 [01:28<00:08,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 367/405 [01:28<00:08,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 368/405 [01:28<00:08,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 369/405 [01:28<00:07,  4.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%|| 370/405 [01:29<00:07,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 371/405 [01:29<00:07,  4.67it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 372/405 [01:29<00:07,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 373/405 [01:29<00:06,  4.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 374/405 [01:29<00:06,  4.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 375/405 [01:30<00:06,  4.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 376/405 [01:30<00:06,  4.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 377/405 [01:30<00:06,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 378/405 [01:30<00:05,  4.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 379/405 [01:31<00:05,  4.61it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 380/405 [01:31<00:05,  4.57it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 381/405 [01:31<00:05,  4.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 382/405 [01:31<00:04,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 383/405 [01:31<00:04,  4.73it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 384/405 [01:32<00:04,  4.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 385/405 [01:32<00:04,  4.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 386/405 [01:32<00:03,  4.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 387/405 [01:32<00:03,  4.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 388/405 [01:32<00:03,  4.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 389/405 [01:33<00:03,  4.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 390/405 [01:33<00:03,  4.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 391/405 [01:33<00:02,  4.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 392/405 [01:33<00:02,  4.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 393/405 [01:34<00:02,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 394/405 [01:34<00:02,  4.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 395/405 [01:34<00:02,  4.54it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 396/405 [01:34<00:01,  4.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 397/405 [01:34<00:01,  4.39it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 398/405 [01:35<00:01,  4.45it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 399/405 [01:35<00:01,  4.53it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 400/405 [01:35<00:01,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 401/405 [01:35<00:00,  4.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 402/405 [01:35<00:00,  4.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 403/405 [01:36<00:00,  4.66it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 404/405 [01:36<00:00,  4.64it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 405/405 [01:36<00:00,  4.20it/s]\n102it [00:23,  4.41it/s]\n"},{"output_type":"stream","name":"stdout","text":"tensor(0.4844, device='cuda:0')\n0.0020298911474564596\ntensor(0.5938, device='cuda:0')\n0.0012309886988075796\ntensor(0.7500, device='cuda:0')\n0.0008036914055827303\ntensor(0.5781, device='cuda:0')\n0.0007036961945396642\ntensor(0.7812, device='cuda:0')\n0.0005337493907506891\ntensor(0.6719, device='cuda:0')\n0.00043643360857154663\ntensor(0.8906, device='cuda:0')\n0.0004074820514817346\ntensor(0.9688, device='cuda:0')\n0.0003794256265290908\ntensor(0.9219, device='cuda:0')\n0.0003055294848320605\ntensor(0.6250, device='cuda:0')\n0.0003159010825280501\ntensor(0.7188, device='cuda:0')\n0.000265302246587719\ntensor(0.7344, device='cuda:0')\n0.00021622811905436452\ntensor(0.7812, device='cuda:0')\n0.00019408553375461235\ntensor(0.9219, device='cuda:0')\n0.00016898602862841435\ntensor(0.8906, device='cuda:0')\n0.00016828484178763256\ntensor(0.9219, device='cuda:0')\n0.0001303252716241173\ntensor(0.7812, device='cuda:0')\n0.00012591525396166836\ntensor(0.9531, device='cuda:0')\n0.0001226331027920514\ntensor(0.8125, device='cuda:0')\n0.00010085752166853823\ntensor(0.9062, device='cuda:0')\n9.099913306452515e-05\n"}],"execution_count":809},{"cell_type":"code","source":"#!g1.1\ntorch.save(student_model.state_dict(), 'student32.pth')","metadata":{"cellId":"j0k1ejf8iceogw6njkmz6","trusted":true},"outputs":[],"execution_count":811},{"cell_type":"code","source":"#!g1.1\nsum([param.numel() for param in student_model.parameters()])","metadata":{"cellId":"d3z0606xiwqf278moryvsp","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"6792"},"metadata":{}}],"execution_count":818},{"cell_type":"code","source":"#!g1.1\nplt.plot(teacher_metrics)\nplt.plot(student_metrics32)\nplt.legend(['Teacher', 'Student distill-2'])\nplt.ylabel('Metric')\nplt.xlabel('Epoch')\nplt.grid()\nplt.show()","metadata":{"cellId":"uq95strefhdhow0v2zh0p","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":814},{"cell_type":"markdown","source":"## + Attention Layer Distillation\n","metadata":{"cellId":"r08nvjc89j6k4uxlowge7"}},{"cell_type":"code","source":"#!g1.1\nclass CRNNwithAttention(CRNN):\n    def __init__(self, config: TaskConfig):\n        super().__init__(config)\n        \n    def forward(self, x):\n        \n        x = x.unsqueeze(dim=1)\n        conv_output = self.conv(x).transpose(-1, -2)\n        gru_output, _ = self.gru(conv_output)\n        alphas = self.attention.energy(gru_output)\n        alphas_softmax = torch.softmax(alphas, dim=-2)\n        cv = (gru_output * alphas_softmax).sum(dim=-2)\n        output = self.classifier(cv)\n        #    -, \n        return output, alphas","metadata":{"cellId":"gftgj1u795laqrcs57qhrv","trusted":true},"outputs":[],"execution_count":767},{"cell_type":"code","source":"#!g1.1\ndef train_epoch_distil_attention(model, student_model, opt, loader, log_melspec, device, alpha=0.5):\n    # alpha -    \n    model.eval()\n    student_model.train()\n    \n    mse_losses = []\n    cres = []\n    mse_atts = []\n    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n        \n        batch, labels = batch.to(device), labels.to(device)\n        batch = log_melspec(batch)\n\n        opt.zero_grad()\n        # run model # with autocast():\n        logits_teacher, alphas_teacher = model(batch)\n        logits_student, alphas_student = student_model(batch)\n        logits_teacher = logits_teacher.detach()\n        alphas_teacher = alphas_teacher.detach()\n        \n        \n        probs = F.softmax(logits_student, dim=-1)\n        mse = F.mse_loss(logits_student, logits_teacher)\n        cre = F.cross_entropy(logits_student, labels)\n        mse_att = F.mse_loss(alphas_teacher, alphas_student)\n        loss = (mse * alpha) + (cre * alpha) + mse_att\n        \n        ###\n        mse_losses.append(mse.detach().cpu().numpy())\n        cres.append(cre.detach().cpu().numpy())\n        mse_atts.append(mse_att.detach().cpu().numpy())\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n\n        opt.step()\n\n        # logging\n        argmax_probs = torch.argmax(probs, dim=-1)\n        FA, FR = count_FA_FR(argmax_probs, labels)\n        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n        ####\n\n\n    return acc, np.mean(mse_losses), np.mean(cres), np.mean(mse_atts)\n\n@torch.no_grad()\ndef val_epoch_distil_attention(model, loader, log_melspec, device):\n    val_losses, accs, FAs, FRs = [], [], [], []\n    all_probs, all_labels = [], []\n    for i, (batch, labels) in tqdm(enumerate(loader)):\n        batch, labels = batch.to(device), labels.to(device)\n        batch = log_melspec(batch)\n\n        output, _ = model(batch)\n        probs = F.softmax(output, dim=-1)\n        loss = F.cross_entropy(output, labels)\n        argmax_probs = torch.argmax(probs, dim=-1)\n        print(probs)\n        all_probs.append(probs[:, 1].cpu())\n        all_labels.append(labels.cpu())\n        val_losses.append(loss.item())\n        accs.append(\n            torch.sum(argmax_probs == labels).item() /  # ???\n            torch.numel(argmax_probs)\n        )\n        FA, FR = count_FA_FR(argmax_probs, labels)\n        FAs.append(FA)\n        FRs.append(FR)\n        au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n        return au_fa_fr","metadata":{"cellId":"wu09xujhmlmzr9yj27keq","trusted":true},"outputs":[],"execution_count":824},{"cell_type":"code","source":"#!g1.1\nconfig = TaskConfig()\nteacher = CRNNwithAttention(config).to(config.device)\nteacher.load_state_dict(torch.load('CRNN_Model.pth'))","metadata":{"cellId":"jhf974vp7mgn1icspci47","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":793},{"cell_type":"code","source":"#!g1.1\nconfig = TaskConfig()\nconfig.cnn_out_channels = 2\nconfig.hidden_size = 20\nconfig.gru_num_layers = 1\nstudent_model = CRNNwithAttention(config).to(config.device)\n","metadata":{"cellId":"bu1l7zjnaov8o2h6lyc68","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n  \"num_layers={}\".format(dropout, num_layers))\n"}],"execution_count":794},{"cell_type":"code","source":"#!g1.1\nprofile(student_model, (torch.randn(1, 40, 101).cuda(), )) ","metadata":{"cellId":"dgurla62hp6iy1w5m5187","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.CRNNwithAttention'>. Treat it as zero Macs and zero Params.\u001B[00m\n"},{"output_type":"display_data","data":{"text/plain":"(84476.0, 4165.0)"},"metadata":{}}],"execution_count":795},{"cell_type":"code","source":"#!g1.1\n\n\nopt = torch.optim.Adam(\n    student_model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\n\nstudent_metrics4 = []\naccs_train = []\ncres_train = []\nmses_train = []\ntrain_ces = []\nmse_atts_list = []\n# TaskConfig.num_epochs\nfor n in range(TaskConfig.num_epochs):\n\n    accs, mse_losses, cres, mse_atts = train_epoch_distil_attention(teacher, student_model, opt, train_loader, melspec_train,\n                                                                    config.device, alpha=0.5)\n\n    au_fa_fr = val_epoch_distil_attention(student_model, val_loader,\n                          melspec_val, config.device)\n    student_metrics4.append(au_fa_fr)\n    mses_train.append(mse_losses)\n    cres_train.append(cres)\n    accs_train.append(accs)\n    mse_atts_list.append(mse_atts)\n    print(n, au_fa_fr)","metadata":{"cellId":"7n63pk94pv91f8kjlnzze7","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"100%|| 405/405 [01:29<00:00,  4.55it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.50it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.54it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.49it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.54it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.46it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.53it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.47it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.54it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.48it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.57it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.49it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.54it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.48it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.53it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.48it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.53it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.50it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.56it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:30<00:00,  4.47it/s]\n0it [00:00, ?it/s]\n"},{"output_type":"stream","name":"stdout","text":"0 1.7088275\n-0.0\n1 1.6633548\n6.103515625e-05\n2 1.6212746\n6.103515625e-05\n3 1.5461599\n6.103515625e-05\n4 1.5418508\n6.103515625e-05\n5 1.4117151\n0.0001220703125\n6 1.4181551\n-0.0\n7 1.3601241\n-0.0\n8 1.2902108\n-0.0\n9 1.2634957\n6.103515625e-05\n10 1.2500287\n0.0001220703125\n11 1.2060695\n6.103515625e-05\n12 1.1710997\n0.0001220703125\n13 1.1641366\n0.00018310546875\n14 1.0755723\n0.0001220703125\n15 1.1371413\n0.0001220703125\n16 1.0968562\n0.00018310546875\n17 1.0841584\n6.103515625e-05\n18 1.0284449\n6.103515625e-05\n19 1.0012472\n6.103515625e-05\n"}],"execution_count":797},{"cell_type":"code","source":"#!g1.1\nval_epoch_distil_attention(student_model, val_loader,\n                          melspec_val, config.device)","metadata":{"cellId":"n5z6ua35awiocjup6siz9h","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[9.9972e-01, 2.7920e-04],\n        [9.9949e-01, 5.0937e-04],\n        [9.9990e-01, 1.0320e-04],\n        [9.9959e-01, 4.1434e-04],\n        [1.1266e-03, 9.9887e-01],\n        [9.9808e-01, 1.9152e-03],\n        [9.9929e-01, 7.0588e-04],\n        [9.8637e-01, 1.3625e-02],\n        [9.9126e-01, 8.7356e-03],\n        [9.9980e-01, 1.9529e-04],\n        [9.7629e-01, 2.3705e-02],\n        [9.9970e-01, 3.0473e-04],\n        [9.9928e-01, 7.2176e-04],\n        [9.9938e-01, 6.2292e-04],\n        [9.9981e-01, 1.9187e-04],\n        [9.9966e-01, 3.3626e-04],\n        [9.9979e-01, 2.0924e-04],\n        [9.9925e-01, 7.5263e-04],\n        [9.9964e-01, 3.6246e-04],\n        [9.9958e-01, 4.2105e-04],\n        [9.8368e-01, 1.6316e-02],\n        [9.9967e-01, 3.2608e-04],\n        [9.9974e-01, 2.6251e-04],\n        [9.9972e-01, 2.7828e-04],\n        [7.2171e-01, 2.7829e-01],\n        [9.9978e-01, 2.1547e-04],\n        [9.9959e-01, 4.1167e-04],\n        [9.9953e-01, 4.6856e-04],\n        [9.9954e-01, 4.5947e-04],\n        [9.9893e-01, 1.0735e-03],\n        [9.9949e-01, 5.1127e-04],\n        [9.8759e-01, 1.2412e-02],\n        [1.9102e-01, 8.0898e-01],\n        [9.9785e-01, 2.1483e-03],\n        [9.9974e-01, 2.6366e-04],\n        [9.9819e-01, 1.8125e-03],\n        [9.9940e-01, 5.9588e-04],\n        [9.9911e-01, 8.8561e-04],\n        [9.9959e-01, 4.1154e-04],\n        [9.9943e-01, 5.6680e-04],\n        [9.9946e-01, 5.3768e-04],\n        [9.9922e-01, 7.7914e-04],\n        [1.1006e-03, 9.9890e-01],\n        [9.9981e-01, 1.8924e-04],\n        [9.9968e-01, 3.1538e-04],\n        [9.7471e-01, 2.5294e-02],\n        [9.9985e-01, 1.5078e-04],\n        [9.9948e-01, 5.2238e-04],\n        [9.9981e-01, 1.9009e-04],\n        [9.9975e-01, 2.5081e-04],\n        [9.9969e-01, 3.1050e-04],\n        [9.9933e-01, 6.7487e-04],\n        [8.9640e-01, 1.0360e-01],\n        [9.9956e-01, 4.4081e-04],\n        [9.9759e-01, 2.4142e-03],\n        [9.3858e-01, 6.1421e-02],\n        [9.9977e-01, 2.3266e-04],\n        [9.9931e-01, 6.8596e-04],\n        [9.9913e-01, 8.7470e-04],\n        [9.9951e-01, 4.9440e-04],\n        [9.9280e-01, 7.1966e-03],\n        [9.9977e-01, 2.3386e-04],\n        [9.9946e-01, 5.3954e-04],\n        [6.1363e-03, 9.9386e-01],\n        [9.9932e-01, 6.8279e-04],\n        [9.9987e-01, 1.2720e-04],\n        [9.9922e-01, 7.7966e-04],\n        [9.9981e-01, 1.9458e-04],\n        [9.9976e-01, 2.4080e-04],\n        [9.5965e-01, 4.0350e-02],\n        [9.9864e-01, 1.3629e-03],\n        [9.3917e-01, 6.0835e-02],\n        [9.9910e-01, 8.9648e-04],\n        [9.9952e-01, 4.7651e-04],\n        [9.9701e-01, 2.9871e-03],\n        [9.9981e-01, 1.8605e-04],\n        [9.9900e-01, 1.0050e-03],\n        [9.9933e-01, 6.7178e-04],\n        [9.9973e-01, 2.6635e-04],\n        [7.8984e-02, 9.2102e-01],\n        [9.9926e-01, 7.3598e-04],\n        [9.9837e-01, 1.6277e-03],\n        [9.9977e-01, 2.2697e-04],\n        [9.9981e-01, 1.8699e-04],\n        [9.9981e-01, 1.8694e-04],\n        [9.9987e-01, 1.2499e-04],\n        [9.9984e-01, 1.6359e-04],\n        [9.9963e-01, 3.7195e-04],\n        [9.9975e-01, 2.5431e-04],\n        [9.9935e-01, 6.4782e-04],\n        [9.5968e-01, 4.0320e-02],\n        [9.9968e-01, 3.1762e-04],\n        [9.9914e-01, 8.6104e-04],\n        [9.9834e-01, 1.6570e-03],\n        [9.9660e-01, 3.3981e-03],\n        [9.9920e-01, 8.0486e-04],\n        [9.9929e-01, 7.0758e-04],\n        [9.4120e-01, 5.8802e-02],\n        [9.2895e-01, 7.1052e-02],\n        [9.9832e-01, 1.6828e-03],\n        [9.9919e-01, 8.1354e-04],\n        [8.3645e-01, 1.6355e-01],\n        [9.9904e-01, 9.6395e-04],\n        [9.9387e-01, 6.1317e-03],\n        [9.9941e-01, 5.8513e-04],\n        [9.9874e-01, 1.2554e-03],\n        [9.9982e-01, 1.7647e-04],\n        [9.9964e-01, 3.5574e-04],\n        [9.9990e-01, 9.6558e-05],\n        [9.8737e-01, 1.2632e-02],\n        [9.8982e-01, 1.0184e-02],\n        [9.9170e-01, 8.2959e-03],\n        [9.8283e-01, 1.7174e-02],\n        [9.9957e-01, 4.3149e-04],\n        [9.9803e-01, 1.9666e-03],\n        [9.8282e-01, 1.7185e-02],\n        [9.9149e-01, 8.5075e-03],\n        [9.9978e-01, 2.1570e-04],\n        [9.8934e-01, 1.0656e-02],\n        [9.9965e-01, 3.4774e-04],\n        [9.9944e-01, 5.5966e-04],\n        [9.9082e-01, 9.1821e-03],\n        [9.9861e-01, 1.3947e-03],\n        [9.9948e-01, 5.1876e-04],\n        [9.9944e-01, 5.6109e-04],\n        [9.9874e-01, 1.2631e-03],\n        [1.7631e-04, 9.9982e-01],\n        [9.9854e-01, 1.4633e-03]], device='cuda:0')\n"},{"output_type":"stream","name":"stderr","text":"0it [00:00, ?it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n0it [00:03, ?it/s]\n"},{"output_type":"display_data","data":{"text/plain":"6.103515625e-05"},"metadata":{}}],"execution_count":825},{"cell_type":"code","source":"#!g1.1\nplt.plot(teacher_metrics)\nplt.plot(student_metrics3)\nplt.plot(student_metrics4)\nplt.legend(['Teacher', 'Student distill base', 'Student distill base + attention'])\nplt.ylabel('Metric')\nplt.xlabel('Epoch')\nplt.grid()\nplt.show()","metadata":{"cellId":"i8n8wwq6qe9hlp1he5pd8w","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":786},{"cell_type":"code","source":"#!g1.1\n\nconfig = TaskConfig()\nteacher = CRNNwithAttention(config).to(config.device)\nteacher.load_state_dict(torch.load('CRNN_Model.pth'))\n\nconfig = TaskConfig()\nconfig.cnn_out_channels = 3\nconfig.kernel_size = (3, 20)\nconfig.hidden_size = 18\nconfig.gru_num_layers = 2\n\n\nstudent_model = CRNNwithAttention(config).to(config.device)\nprofile(student_model, (torch.randn(1, 40, 101).cuda(), )) ","metadata":{"cellId":"9meflsc8y5vtemn6a289x","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001B[00m\n\u001B[91m[WARN] Cannot find rule for <class '__main__.CRNNwithAttention'>. Treat it as zero Macs and zero Params.\u001B[00m\n"},{"output_type":"display_data","data":{"text/plain":"(113127.0, 6792.0)"},"metadata":{}}],"execution_count":819},{"cell_type":"code","source":"#!g1.1\n#!g1.1\nopt = torch.optim.Adam(\n    student_model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\n\nstudent_metrics4 = []\naccs_train = []\ncres_train = []\nmses_train = []\ntrain_ces = []\nmse_atts_list = []\n# TaskConfig.num_epochs\nfor n in range(TaskConfig.num_epochs):\n\n    accs, mse_losses, cres, mse_atts = train_epoch_distil_attention(teacher, student_model, opt, train_loader, melspec_train,\n                                                                    config.device, alpha=0.5)\n\n    au_fa_fr = val_epoch_distil_attention(student_model, val_loader,\n                          melspec_val, config.device)\n    student_metrics4.append(au_fa_fr)\n    mses_train.append(mse_losses)\n    cres_train.append(cres)\n    accs_train.append(accs)\n    mse_atts_list.append(mse_atts)\n    print(n, au_fa_fr)\n    \ntorch.save(student_model.state_dict(), 'student4.pth')","metadata":{"cellId":"du2x60gbdhj2udm16pr8","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"100%|| 405/405 [01:27<00:00,  4.61it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.71it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.67it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.60it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.67it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:27<00:00,  4.61it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.68it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.67it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:27<00:00,  4.63it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.69it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:27<00:00,  4.62it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.68it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:27<00:00,  4.65it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.70it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:27<00:00,  4.63it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.69it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.70it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:27<00:00,  4.63it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:26<00:00,  4.71it/s]\n0it [00:00, ?it/s]\n"},{"output_type":"stream","name":"stdout","text":"0 0.00146484375\n1 0.00048828125\n2 0.0003662109375\n3 6.103515625e-05\n4 0.0001220703125\n5 0.0001220703125\n6 6.103515625e-05\n7 6.103515625e-05\n8 6.103515625e-05\n9 -0.0\n10 -0.0\n11 -0.0\n12 -0.0\n13 -0.0\n14 -0.0\n15 6.103515625e-05\n16 -0.0\n17 -0.0\n18 -0.0\n19 -0.0\n"}],"execution_count":820},{"cell_type":"code","source":"#!g1.1\ntorch.save(student_model.state_dict(), 'student4.pth')","metadata":{"cellId":"2a9c5p40mlprxrjsu6iyos"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"twclpy263ejk3958y3dth"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nopt = torch.optim.Adam(\n    student_model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\n\nstudent_metrics4 = []\naccs_train = []\ncres_train = []\nmses_train = []\ntrain_ces = []\nmse_atts_list = []\n# TaskConfig.num_epochs\nfor n in range(TaskConfig.num_epochs):\n\n    accs, mse_losses, cres, mse_atts = train_epoch_distil_attention(teacher, student_model, opt, train_loader, melspec_train,\n                                                                    config.device, alpha=0.5)\n\n    au_fa_fr = val_epoch_distil_attention(student_model, val_loader,\n                          melspec_val, config.device)\n    student_metrics4.append(au_fa_fr)\n    mses_train.append(mse_losses)\n    cres_train.append(cres)\n    accs_train.append(accs)\n    mse_atts_list.append(mse_atts)\n    print(n, mse_losses)\n    print(au_fa_fr)","metadata":{"cellId":"jl123onc01q2tck7yb1r8c","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"100%|| 405/405 [01:30<00:00,  4.49it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.51it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.51it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.57it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.51it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.51it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.51it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.57it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.52it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.53it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.55it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.52it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:28<00:00,  4.58it/s]\n0it [00:00, ?it/s]\n100%|| 405/405 [01:29<00:00,  4.53it/s]\n0it [00:00, ?it/s]\n"},{"output_type":"stream","name":"stdout","text":"0 6.9679656\n0.0048828125\n1 7.0058846\n0.00079345703125\n2 6.4730263\n0.00018310546875\n3 5.5014467\n0.0003662109375\n4 4.428193\n0.000244140625\n5 3.449133\n0.0003662109375\n6 2.6224043\n0.00042724609375\n7 2.1492608\n0.000244140625\n8 1.8911475\n0.00018310546875\n9 1.6401012\n0.000244140625\n10 1.4984434\n0.0001220703125\n11 1.4478315\n0.0001220703125\n12 1.3731737\n0.0001220703125\n13 1.2713467\n6.103515625e-05\n14 1.1897453\n6.103515625e-05\n15 1.0902601\n6.103515625e-05\n16 1.0931926\n6.103515625e-05\n17 1.0410961\n-0.0\n18 0.986872\n6.103515625e-05\n19 0.9557533\n6.103515625e-05\n"}],"execution_count":804},{"cell_type":"code","source":"#!g1.1\nval_epoch_distil_attention(student_model, val_loader,\n                          melspec_val, config.device)","metadata":{"cellId":"6hqen52q82k7rf7wgd4b15"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ntorch.save(student_model.state_dict(), 'student4.pth')","metadata":{"cellId":"2wk9ga5wnce34a1702hhpy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n#!g1.1\nplt.plot(teacher_metrics)\nplt.plot(student_metrics4)\nplt.legend(['Teacher', 'Student distill base + attention'])\nplt.ylabel('Metric')\nplt.xlabel('Epoch')\nplt.grid()\nplt.show()","metadata":{"cellId":"67froqvu4kf8rsmr65ngu4","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":805},{"cell_type":"code","source":"#!g1.1\nfor n in range(TaskConfig.num_epochs):\n\n    accs, mse_losses, cres, mse_atts = train_epoch_distil_attention(teacher, student_model, opt, train_loader, melspec_train,\n                                                                    config.device, alpha=0.5)\n\n    au_fa_fr = val_epoch_distil_attention(student_model, val_loader,\n                          melspec_val, config.device)\n    student_metrics4.append(au_fa_fr)\n    mses_train.append(mse_losses)\n    cres_train.append(cres)\n    accs_train.append(accs)\n    mse_atts_list.append(mse_atts)\n    print(n, au_fa_fr)\n    \ntorch.save(student_model.state_dict(), 'student5.pth')","metadata":{"cellId":"rfldqmx72uqe87jivgdnr","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"s means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 281/405 [00:46<00:20,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 282/405 [00:46<00:20,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 283/405 [00:47<00:20,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 284/405 [00:47<00:19,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 285/405 [00:47<00:19,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 286/405 [00:47<00:19,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 287/405 [00:47<00:19,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 288/405 [00:47<00:19,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|  | 289/405 [00:48<00:18,  6.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 290/405 [00:48<00:18,  6.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 291/405 [00:48<00:18,  6.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 292/405 [00:48<00:18,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 293/405 [00:48<00:17,  6.24it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 294/405 [00:48<00:17,  6.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 295/405 [00:49<00:17,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 296/405 [00:49<00:17,  6.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 297/405 [00:49<00:17,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 298/405 [00:49<00:17,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 299/405 [00:49<00:17,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 300/405 [00:49<00:17,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 301/405 [00:50<00:16,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 302/405 [00:50<00:16,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 303/405 [00:50<00:16,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 304/405 [00:50<00:16,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 305/405 [00:50<00:16,  6.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 306/405 [00:50<00:16,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 307/405 [00:51<00:15,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 308/405 [00:51<00:15,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 309/405 [00:51<00:16,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 310/405 [00:51<00:15,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 311/405 [00:51<00:15,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 312/405 [00:51<00:15,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 313/405 [00:52<00:14,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 314/405 [00:52<00:15,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 315/405 [00:52<00:14,  6.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 316/405 [00:52<00:14,  6.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 317/405 [00:52<00:14,  6.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 318/405 [00:52<00:14,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 319/405 [00:53<00:14,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 320/405 [00:53<00:14,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 321/405 [00:53<00:13,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 322/405 [00:53<00:13,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 323/405 [00:53<00:13,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 324/405 [00:53<00:13,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 325/405 [00:54<00:13,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 326/405 [00:54<00:12,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 327/405 [00:54<00:12,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 328/405 [00:54<00:13,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 329/405 [00:54<00:12,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%| | 330/405 [00:54<00:12,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 331/405 [00:55<00:12,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 332/405 [00:55<00:12,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 333/405 [00:55<00:11,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 334/405 [00:55<00:11,  6.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 335/405 [00:55<00:11,  6.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 336/405 [00:55<00:11,  6.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 337/405 [00:55<00:10,  6.32it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 338/405 [00:56<00:10,  6.27it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 339/405 [00:56<00:10,  6.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 340/405 [00:56<00:10,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 341/405 [00:57<00:30,  2.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 342/405 [00:57<00:23,  2.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 343/405 [00:57<00:19,  3.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 344/405 [00:58<00:16,  3.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 345/405 [00:58<00:14,  4.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 346/405 [00:58<00:12,  4.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 347/405 [00:58<00:11,  4.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 348/405 [00:58<00:10,  5.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 349/405 [00:58<00:10,  5.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 350/405 [00:59<00:09,  5.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 351/405 [00:59<00:09,  5.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 352/405 [00:59<00:09,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 353/405 [00:59<00:08,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 354/405 [00:59<00:08,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 355/405 [00:59<00:08,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 356/405 [01:00<00:08,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 357/405 [01:00<00:07,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 358/405 [01:00<00:07,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 359/405 [01:00<00:07,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 360/405 [01:00<00:07,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 361/405 [01:00<00:07,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 362/405 [01:01<00:07,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 363/405 [01:01<00:06,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 364/405 [01:01<00:06,  6.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 365/405 [01:01<00:06,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 366/405 [01:01<00:06,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 367/405 [01:01<00:06,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 368/405 [01:02<00:06,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 369/405 [01:02<00:05,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%|| 370/405 [01:02<00:05,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 371/405 [01:02<00:05,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 372/405 [01:02<00:05,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 373/405 [01:02<00:05,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 374/405 [01:03<00:05,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 375/405 [01:03<00:05,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 376/405 [01:03<00:04,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 377/405 [01:03<00:04,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 378/405 [01:03<00:04,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 379/405 [01:03<00:04,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 380/405 [01:04<00:04,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 381/405 [01:04<00:03,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 382/405 [01:04<00:03,  5.87it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 383/405 [01:04<00:03,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 384/405 [01:04<00:03,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 385/405 [01:04<00:03,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 386/405 [01:05<00:03,  5.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 387/405 [01:05<00:03,  5.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 388/405 [01:05<00:03,  5.64it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 389/405 [01:05<00:02,  5.62it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 390/405 [01:05<00:02,  5.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 391/405 [01:06<00:02,  5.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 392/405 [01:06<00:02,  5.63it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 393/405 [01:06<00:02,  5.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 394/405 [01:06<00:01,  5.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 395/405 [01:06<00:01,  5.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 396/405 [01:06<00:01,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 397/405 [01:07<00:01,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 398/405 [01:07<00:01,  5.87it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 399/405 [01:07<00:00,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 400/405 [01:07<00:00,  5.79it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 401/405 [01:07<00:00,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 402/405 [01:07<00:00,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 403/405 [01:08<00:00,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 404/405 [01:08<00:00,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 405/405 [01:08<00:00,  5.93it/s]\n0it [00:00, ?it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n0it [00:00, ?it/s]\n  0%|          | 0/405 [00:00<?, ?it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  0%|          | 1/405 [00:00<01:12,  5.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  0%|          | 2/405 [00:00<01:11,  5.65it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 3/405 [00:00<01:09,  5.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 4/405 [00:00<01:08,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|          | 5/405 [00:00<01:06,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  1%|         | 6/405 [00:01<01:07,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 7/405 [00:01<01:07,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 8/405 [00:01<01:06,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 9/405 [00:01<01:06,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  2%|         | 10/405 [00:01<01:05,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 11/405 [00:01<01:06,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 12/405 [00:02<01:04,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 13/405 [00:02<01:03,  6.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  3%|         | 14/405 [00:02<01:04,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 15/405 [00:02<01:02,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 16/405 [00:02<01:03,  6.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 17/405 [00:02<01:02,  6.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  4%|         | 18/405 [00:02<01:02,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 19/405 [00:03<01:02,  6.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 20/405 [00:03<01:02,  6.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 21/405 [00:03<01:02,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  5%|         | 22/405 [00:03<01:02,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 23/405 [00:03<01:04,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 24/405 [00:03<01:02,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 25/405 [00:04<01:03,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  6%|         | 26/405 [00:04<01:02,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 27/405 [00:04<01:02,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 28/405 [00:04<01:01,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 29/405 [00:04<01:00,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  7%|         | 30/405 [00:04<01:01,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 31/405 [00:05<01:02,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 32/405 [00:05<01:00,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 33/405 [00:05<01:01,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  8%|         | 34/405 [00:05<01:02,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 35/405 [00:05<01:04,  5.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 36/405 [00:05<01:03,  5.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 37/405 [00:06<01:06,  5.52it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n  9%|         | 38/405 [00:06<01:05,  5.60it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 39/405 [00:06<01:07,  5.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 40/405 [00:06<01:05,  5.58it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 41/405 [00:06<01:03,  5.71it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 10%|         | 42/405 [00:07<01:02,  5.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 43/405 [00:07<01:00,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 44/405 [00:07<01:01,  5.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|         | 45/405 [00:07<01:01,  5.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 11%|        | 46/405 [00:07<01:00,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 47/405 [00:07<00:59,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 48/405 [00:08<00:58,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 49/405 [00:08<00:59,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 12%|        | 50/405 [00:08<00:58,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 51/405 [00:08<00:58,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 52/405 [00:08<00:58,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 53/405 [00:08<00:57,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 13%|        | 54/405 [00:09<00:57,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 55/405 [00:09<00:57,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 56/405 [00:09<00:57,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 57/405 [00:09<00:56,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 14%|        | 58/405 [00:09<00:56,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 59/405 [00:09<00:55,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 60/405 [00:10<00:55,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 61/405 [00:10<00:55,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 15%|        | 62/405 [00:10<00:55,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 63/405 [00:10<00:56,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 64/405 [00:10<00:55,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 65/405 [00:10<00:56,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 16%|        | 66/405 [00:11<00:56,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 67/405 [00:11<00:57,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 68/405 [00:11<00:56,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 69/405 [00:11<00:57,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 17%|        | 70/405 [00:11<00:57,  5.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 71/405 [00:11<00:57,  5.77it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 72/405 [00:12<00:57,  5.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 73/405 [00:12<00:57,  5.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 18%|        | 74/405 [00:12<00:56,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 75/405 [00:12<00:54,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 76/405 [00:12<00:55,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 77/405 [00:12<00:55,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 19%|        | 78/405 [00:13<00:55,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 79/405 [00:13<00:53,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 80/405 [00:13<00:54,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 81/405 [00:13<00:53,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 82/405 [00:13<00:53,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 20%|        | 83/405 [00:13<00:54,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 84/405 [00:14<00:54,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 85/405 [00:14<00:53,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|        | 86/405 [00:14<00:53,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 21%|       | 87/405 [00:14<00:52,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 88/405 [00:14<00:52,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 89/405 [00:14<00:52,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 90/405 [00:15<00:52,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 22%|       | 91/405 [00:15<00:51,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 92/405 [00:15<00:53,  5.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 93/405 [00:15<00:53,  5.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 94/405 [00:15<00:51,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 23%|       | 95/405 [00:15<00:50,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 96/405 [00:16<00:52,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 97/405 [00:16<00:53,  5.80it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 98/405 [00:16<00:52,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 24%|       | 99/405 [00:16<00:51,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 100/405 [00:16<00:50,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 101/405 [00:16<00:50,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 102/405 [00:17<00:50,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 25%|       | 103/405 [00:17<00:51,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 104/405 [00:17<00:50,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 105/405 [00:17<00:49,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 106/405 [00:17<00:49,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 26%|       | 107/405 [00:17<00:48,  6.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 108/405 [00:18<00:48,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 109/405 [00:18<00:47,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 110/405 [00:18<00:47,  6.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 27%|       | 111/405 [00:18<00:46,  6.28it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 112/405 [00:18<00:46,  6.31it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 113/405 [00:18<00:47,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 114/405 [00:19<00:48,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 28%|       | 115/405 [00:19<00:48,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 116/405 [00:19<00:47,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 117/405 [00:19<00:46,  6.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 118/405 [00:19<00:46,  6.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 29%|       | 119/405 [00:19<00:46,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 120/405 [00:20<00:46,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 121/405 [00:20<00:46,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 122/405 [00:20<00:45,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 30%|       | 123/405 [00:20<00:45,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 124/405 [00:20<00:45,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 125/405 [00:20<00:45,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|       | 126/405 [00:20<00:45,  6.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 31%|      | 127/405 [00:21<00:45,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 128/405 [00:21<00:46,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 129/405 [00:21<00:48,  5.69it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 130/405 [00:21<00:47,  5.75it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 32%|      | 131/405 [00:21<00:46,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 132/405 [00:22<00:45,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 133/405 [00:22<00:45,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 134/405 [00:22<00:45,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 33%|      | 135/405 [00:22<00:44,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 136/405 [00:22<00:43,  6.21it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 137/405 [00:22<00:42,  6.25it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 138/405 [00:22<00:42,  6.26it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 34%|      | 139/405 [00:23<00:43,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 140/405 [00:23<00:43,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 141/405 [00:23<00:43,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 142/405 [00:23<00:43,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 35%|      | 143/405 [00:23<00:43,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 144/405 [00:23<00:43,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 145/405 [00:24<00:43,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 146/405 [00:24<00:43,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 36%|      | 147/405 [00:24<00:43,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 148/405 [00:24<00:43,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 149/405 [00:24<00:43,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 150/405 [00:24<00:42,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 37%|      | 151/405 [00:25<00:41,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 152/405 [00:25<00:42,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 153/405 [00:25<00:41,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 154/405 [00:25<00:41,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 38%|      | 155/405 [00:25<00:41,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 156/405 [00:25<00:41,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 157/405 [00:26<00:42,  5.87it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 158/405 [00:26<00:41,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 39%|      | 159/405 [00:26<00:41,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 160/405 [00:26<00:41,  5.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 161/405 [00:26<00:40,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 162/405 [00:26<00:39,  6.23it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 163/405 [00:27<00:38,  6.22it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 40%|      | 164/405 [00:27<00:37,  6.34it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 165/405 [00:27<00:37,  6.35it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 166/405 [00:27<00:39,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|      | 167/405 [00:27<00:39,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 41%|     | 168/405 [00:27<00:39,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 169/405 [00:28<00:38,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 170/405 [00:28<00:38,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 171/405 [00:28<00:38,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 42%|     | 172/405 [00:28<00:37,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 173/405 [00:28<00:37,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 174/405 [00:28<00:38,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 175/405 [00:29<00:38,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 43%|     | 176/405 [00:29<00:38,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 177/405 [00:29<00:37,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 178/405 [00:29<00:37,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 179/405 [00:29<00:36,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 44%|     | 180/405 [00:29<00:37,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 181/405 [00:30<00:37,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 182/405 [00:30<00:37,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 183/405 [00:30<00:36,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 45%|     | 184/405 [00:30<00:36,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 185/405 [00:30<00:36,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 186/405 [00:30<00:37,  5.87it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 187/405 [00:31<00:36,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 46%|     | 188/405 [00:31<00:35,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 189/405 [00:31<00:35,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 190/405 [00:31<00:35,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 191/405 [00:31<00:35,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 47%|     | 192/405 [00:31<00:34,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 193/405 [00:32<00:34,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 194/405 [00:32<00:34,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 195/405 [00:32<00:34,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 48%|     | 196/405 [00:32<00:34,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 197/405 [00:32<00:33,  6.19it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 198/405 [00:32<00:33,  6.17it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 199/405 [00:33<00:33,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 49%|     | 200/405 [00:33<00:34,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 201/405 [00:33<00:33,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 202/405 [00:33<00:33,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 203/405 [00:33<00:33,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 50%|     | 204/405 [00:33<00:33,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 205/405 [00:34<00:33,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 206/405 [00:34<00:33,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|     | 207/405 [00:34<00:32,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 51%|    | 208/405 [00:34<00:32,  6.15it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 209/405 [00:34<00:32,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 210/405 [00:34<00:31,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 211/405 [00:35<00:32,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 52%|    | 212/405 [00:35<00:32,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 213/405 [00:35<00:31,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 214/405 [00:35<00:31,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 215/405 [00:35<00:31,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 53%|    | 216/405 [00:35<00:31,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 217/405 [00:36<00:31,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 218/405 [00:36<00:32,  5.73it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 219/405 [00:36<00:33,  5.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 54%|    | 220/405 [00:36<00:32,  5.77it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 221/405 [00:36<00:30,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 222/405 [00:36<00:30,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 223/405 [00:37<00:30,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 55%|    | 224/405 [00:37<00:29,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 225/405 [00:37<00:29,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 226/405 [00:37<00:28,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 227/405 [00:37<00:29,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 56%|    | 228/405 [00:37<00:30,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 229/405 [00:38<00:29,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 230/405 [00:38<00:29,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 231/405 [00:38<00:28,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 57%|    | 232/405 [00:38<00:28,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 233/405 [00:38<00:28,  6.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 234/405 [00:38<00:28,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 235/405 [00:39<00:28,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 58%|    | 236/405 [00:39<00:27,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 237/405 [00:39<00:27,  6.20it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 238/405 [00:39<00:27,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 239/405 [00:39<00:27,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 59%|    | 240/405 [00:39<00:26,  6.16it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 241/405 [00:40<00:27,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 242/405 [00:40<00:26,  6.09it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 243/405 [00:40<00:26,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 244/405 [00:40<00:26,  6.12it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 60%|    | 245/405 [00:40<00:26,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 246/405 [00:40<00:25,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 247/405 [00:41<00:25,  6.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|    | 248/405 [00:41<00:25,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 61%|   | 249/405 [00:41<00:25,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 250/405 [00:41<00:25,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 251/405 [00:41<00:25,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 252/405 [00:41<00:25,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 62%|   | 253/405 [00:42<00:25,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 254/405 [00:42<00:24,  6.11it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 255/405 [00:42<00:24,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 256/405 [00:42<00:24,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 63%|   | 257/405 [00:42<00:24,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 258/405 [00:42<00:24,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 259/405 [00:43<00:24,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 260/405 [00:43<00:24,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 64%|   | 261/405 [00:43<00:24,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 262/405 [00:43<00:23,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 263/405 [00:43<00:23,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 264/405 [00:43<00:23,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 65%|   | 265/405 [00:44<00:23,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 266/405 [00:44<00:23,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 267/405 [00:44<00:22,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 268/405 [00:44<00:22,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 66%|   | 269/405 [00:44<00:22,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 270/405 [00:44<00:22,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 271/405 [00:45<00:22,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 272/405 [00:45<00:22,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 67%|   | 273/405 [00:45<00:21,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 274/405 [00:45<00:21,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 275/405 [00:45<00:21,  6.10it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 276/405 [00:45<00:21,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 68%|   | 277/405 [00:45<00:21,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 278/405 [00:46<00:21,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 279/405 [00:46<00:22,  5.59it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 280/405 [00:46<00:21,  5.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 69%|   | 281/405 [00:46<00:21,  5.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 282/405 [00:46<00:21,  5.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 283/405 [00:47<00:20,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 284/405 [00:47<00:20,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 70%|   | 285/405 [00:47<00:20,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 286/405 [00:47<00:19,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 287/405 [00:47<00:19,  6.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|   | 288/405 [00:47<00:19,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 71%|  | 289/405 [00:48<00:19,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 290/405 [00:48<00:18,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 291/405 [00:49<00:56,  2.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 292/405 [00:49<00:45,  2.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 72%|  | 293/405 [00:49<00:36,  3.04it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 294/405 [00:49<00:31,  3.56it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 295/405 [00:50<00:27,  4.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 296/405 [00:50<00:24,  4.44it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 73%|  | 297/405 [00:50<00:22,  4.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 298/405 [00:50<00:20,  5.18it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 299/405 [00:50<00:19,  5.36it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 300/405 [00:50<00:18,  5.55it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 74%|  | 301/405 [00:51<00:18,  5.68it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 302/405 [00:51<00:17,  5.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 303/405 [00:51<00:17,  5.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 304/405 [00:51<00:17,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 75%|  | 305/405 [00:51<00:16,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 306/405 [00:51<00:16,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 307/405 [00:52<00:16,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 308/405 [00:52<00:16,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 76%|  | 309/405 [00:52<00:16,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 310/405 [00:52<00:15,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 311/405 [00:52<00:16,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 312/405 [00:52<00:15,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 77%|  | 313/405 [00:53<00:15,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 314/405 [00:53<00:15,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 315/405 [00:53<00:15,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 316/405 [00:53<00:14,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 78%|  | 317/405 [00:53<00:14,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 318/405 [00:54<00:14,  5.85it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 319/405 [00:54<00:14,  5.87it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 320/405 [00:54<00:14,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 79%|  | 321/405 [00:54<00:14,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 322/405 [00:54<00:14,  5.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 323/405 [00:54<00:13,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 324/405 [00:55<00:13,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 325/405 [00:55<00:13,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 80%|  | 326/405 [00:55<00:13,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 327/405 [00:55<00:12,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 328/405 [00:55<00:12,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%|  | 329/405 [00:55<00:12,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 81%| | 330/405 [00:56<00:12,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 331/405 [00:56<00:12,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 332/405 [00:56<00:12,  5.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 333/405 [00:56<00:12,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 82%| | 334/405 [00:56<00:12,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 335/405 [00:56<00:11,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 336/405 [00:57<00:12,  5.73it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 337/405 [00:57<00:11,  5.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 83%| | 338/405 [00:57<00:11,  5.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 339/405 [00:57<00:11,  5.74it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 340/405 [00:57<00:11,  5.50it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 341/405 [00:57<00:11,  5.70it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 84%| | 342/405 [00:58<00:11,  5.72it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 343/405 [00:58<00:10,  5.77it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 344/405 [00:58<00:10,  5.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 345/405 [00:58<00:10,  5.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 85%| | 346/405 [00:58<00:10,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 347/405 [00:58<00:10,  5.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 348/405 [00:59<00:09,  5.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 349/405 [00:59<00:09,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 86%| | 350/405 [00:59<00:09,  5.83it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 351/405 [00:59<00:09,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 352/405 [00:59<00:08,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 353/405 [00:59<00:08,  5.94it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 87%| | 354/405 [01:00<00:08,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 355/405 [01:00<00:08,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 356/405 [01:00<00:08,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 357/405 [01:00<00:08,  5.91it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 88%| | 358/405 [01:00<00:07,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 359/405 [01:00<00:07,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 360/405 [01:01<00:07,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 361/405 [01:01<00:07,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 89%| | 362/405 [01:01<00:07,  6.05it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 363/405 [01:01<00:06,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 364/405 [01:01<00:06,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 365/405 [01:01<00:06,  6.13it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 90%| | 366/405 [01:02<00:06,  6.14it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 367/405 [01:02<00:06,  6.07it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 368/405 [01:02<00:06,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%| | 369/405 [01:02<00:06,  5.92it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 91%|| 370/405 [01:02<00:05,  6.03it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 371/405 [01:02<00:05,  5.99it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 372/405 [01:03<00:05,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 373/405 [01:03<00:05,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 92%|| 374/405 [01:03<00:05,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 375/405 [01:03<00:05,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 376/405 [01:03<00:04,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 377/405 [01:03<00:04,  5.89it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 93%|| 378/405 [01:04<00:04,  5.78it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 379/405 [01:04<00:04,  5.88it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 380/405 [01:04<00:04,  5.86it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 381/405 [01:04<00:04,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 94%|| 382/405 [01:04<00:03,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 383/405 [01:05<00:03,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 384/405 [01:05<00:03,  5.90it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 385/405 [01:05<00:03,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 95%|| 386/405 [01:05<00:03,  6.06it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 387/405 [01:05<00:02,  6.08it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 388/405 [01:05<00:02,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 389/405 [01:06<00:02,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 96%|| 390/405 [01:06<00:02,  6.02it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 391/405 [01:06<00:02,  6.01it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 392/405 [01:06<00:02,  5.67it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 393/405 [01:06<00:02,  5.77it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 97%|| 394/405 [01:06<00:01,  5.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 395/405 [01:07<00:01,  5.84it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 396/405 [01:07<00:01,  5.97it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 397/405 [01:07<00:01,  5.96it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 98%|| 398/405 [01:07<00:01,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 399/405 [01:07<00:01,  5.93it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 400/405 [01:07<00:00,  5.82it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 401/405 [01:08<00:00,  5.81it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n 99%|| 402/405 [01:08<00:00,  5.98it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 403/405 [01:08<00:00,  6.00it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 404/405 [01:08<00:00,  5.95it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n100%|| 405/405 [01:08<00:00,  5.90it/s]\n0it [00:00, ?it/s]/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:838: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:924.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n0it [00:00, ?it/s]\n"},{"output_type":"stream","name":"stdout","text":"0 -0.0\n1 -0.0\n2 -0.0\n3 -0.0\n4 -0.0\n5 -0.0\n6 -0.0\n7 -0.0\n8 -0.0\n9 -0.0\n10 -0.0\n11 -0.0\n12 -0.0\n13 -0.0\n14 -0.0\n15 -0.0\n16 -0.0\n17 -0.0\n18 -0.0\n19 -0.0\n"}],"execution_count":821},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"tmy8recjrlmktbq5i86rgl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nplt.plot(teacher_metrics)\nplt.plot(student_metrics4)\nplt.legend(['Teacher', 'Student distill base + attention'])\nplt.ylabel('Metric')\nplt.xlabel('Epoch')\nplt.grid()\nplt.show()","metadata":{"cellId":"3p2tqhozlt4yzqqbw32ok8","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":822},{"cell_type":"markdown","source":"# Quantization","metadata":{"cellId":"gnbe07a9pafzzgpn08l5"}},{"cell_type":"code","source":"#!g1.1\nstudent_model.load_state_dict(torch.load('student4.pth'))\nmodelq = torch.quantization.quantize_dynamic(\n    student_model.to('cpu'),  # the original model\n    {nn.Linear, nn.Conv2d, nn.GRU},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)\nmodelq","metadata":{"cellId":"nck0472w7tieiu2qrfv7f","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"CRNNwithAttention(\n  (conv): Sequential(\n    (0): Conv2d(1, 3, kernel_size=(3, 20), stride=(2, 8))\n    (1): Flatten(start_dim=1, end_dim=2)\n  )\n  (gru): DynamicQuantizedGRU(57, 18, num_layers=2, batch_first=True, dropout=0.1)\n  (attention): Attention(\n    (energy): Sequential(\n      (0): DynamicQuantizedLinear(in_features=18, out_features=18, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n      (1): Tanh()\n      (2): DynamicQuantizedLinear(in_features=18, out_features=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n    )\n  )\n  (classifier): DynamicQuantizedLinear(in_features=18, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n)"},"metadata":{}}],"execution_count":833},{"cell_type":"code","source":"#!g1.1\nconfig = TaskConfig()\nconfig.device='cpu'","metadata":{"cellId":"4x5oyg7vo2c7z0s8p5y6mu","trusted":true},"outputs":[],"execution_count":848},{"cell_type":"code","source":"#!g1.1\nmelspec_train = LogMelspec(is_train=True, config=config)\nmelspec_val = LogMelspec(is_train=False, config=config)\n","metadata":{"cellId":"glqeuah59ohhwbtz010feg","trusted":true},"outputs":[],"execution_count":849},{"cell_type":"code","source":"#!g1.1\nval_epoch_distil_attention(modelq, val_loader,\n                          melspec_val, 'cpu')","metadata":{"execution_id":"667644ec-3e5f-4471-a429-177286ee27ed","cellId":"31we90la2vrone8pkmfi5j","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"1mdr0h0uu1fqij90tr991n"},"outputs":[],"execution_count":null}]}